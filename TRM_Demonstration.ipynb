{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weagan/Tiny-Recursive-Models/blob/main/TRM_Demonstration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukMlLXk54MYC"
      },
      "source": [
        "# Understanding Tiny Recursive Models (TRM)\n",
        "\n",
        "This Colab notebook provides a hands-on implementation of a Tiny Recursive Model (TRM), based on the concepts outlined in the [learn-tiny-recursive-models GitHub repository](https://github.com/vukrosic/learn-tiny-recursive-models).\n",
        "\n",
        "We will:\n",
        "1.  Implement the core `RecursiveBlock`.\n",
        "2.  Build the full `TRM` model.\n",
        "3.  Run a forward pass to see how it processes a sequence.\n",
        "4.  Set up a simple training loop to watch the model learn.\n"
      ],
      "id": "ukMlLXk54MYC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W9KuTXn4MYF"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's ensure we have PyTorch installed. Google Colab usually comes with it pre-installed, but it's good practice to run the installation command.\n"
      ],
      "id": "_W9KuTXn4MYF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RWheeaV4MYF",
        "outputId": "62cb3aa6-fa25-4c53-9dc1-ac5ed7a2ab9f"
      },
      "source": [
        "!pip install -q torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "1RWheeaV4MYF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tr1PJ4H4MYG"
      },
      "source": [
        "## 1. The Core Component: `RecursiveBlock`\n",
        "\n",
        "The fundamental building block of a TRM is the `RecursiveBlock`. It takes an input tensor and a hidden state from the previous step, and produces an output and an updated hidden state. This recursive nature allows it to process sequences step-by-step.\n",
        "\n",
        "The block consists of:\n",
        "-   Two Layer Normalization layers for stability.\n",
        "-   Two Linear layers to transform the concatenated input and state.\n",
        "-   A GELU activation function for non-linearity.\n"
      ],
      "id": "9Tr1PJ4H4MYG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT8pOB5c4MYH"
      },
      "source": [
        "class RecursiveBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single recursive block for the TRM.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Layer normalization for the state and input\n",
        "        self.norm_state = nn.LayerNorm(d_model)\n",
        "        self.norm_input = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Linear layers to process the combined state and input\n",
        "        # The input dimension is 2 * d_model because we concatenate state and input\n",
        "        self.linear1 = nn.Linear(2 * d_model, 2 * d_model)\n",
        "        self.activation = nn.GELU()\n",
        "        self.linear2 = nn.Linear(2 * d_model, 2 * d_model)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        \"\"\"\n",
        "        Forward pass for the recursive block.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor for the current step. Shape: (batch_size, d_model)\n",
        "            state (torch.Tensor): Hidden state from the previous step. Shape: (batch_size, d_model)\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: The output and the new state. Both have shape (batch_size, d_model)\n",
        "        \"\"\"\n",
        "        # Normalize the state and input separately\n",
        "        normalized_state = self.norm_state(state)\n",
        "        normalized_input = self.norm_input(x)\n",
        "\n",
        "        # Concatenate along the feature dimension\n",
        "        combined_input = torch.cat([normalized_state, normalized_input], dim=1)\n",
        "\n",
        "        # Pass through the linear layers\n",
        "        hidden = self.linear1(combined_input)\n",
        "        hidden = self.activation(hidden)\n",
        "        processed_output = self.linear2(hidden)\n",
        "\n",
        "        # The magic of TRM: the new state and output are derived from the same processed tensor.\n",
        "        # This is a simple but effective way to update the state.\n",
        "        new_state = state + processed_output[:, :self.d_model]\n",
        "        output = processed_output[:, self.d_model:]\n",
        "\n",
        "        return output, new_state\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "cT8pOB5c4MYH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZfdnDD4MYI"
      },
      "source": [
        "## 2. The TRM Model: Processing Sequences\n",
        "\n",
        "The full TRM model wraps the `RecursiveBlock`. It initializes the hidden state (usually with zeros) and then iterates through the input sequence, feeding each element into the recursive block one at a time. This step-by-step processing is the core of its operation.\n",
        "\n",
        "We also add input and output embedding layers to map our vocabulary to the model's dimension and back.\n"
      ],
      "id": "_YZfdnDD4MYI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJaPYCJr4MYJ"
      },
      "source": [
        "class TRM(nn.Module):\n",
        "    \"\"\"\n",
        "    The Tiny Recursive Model.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Embedding layer to convert token IDs to vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # The recursive block\n",
        "        self.recursive_block = RecursiveBlock(d_model)\n",
        "\n",
        "        # A final layer normalization and linear layer to produce logits\n",
        "        self.norm_out = nn.LayerNorm(d_model)\n",
        "        self.output_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        \"\"\"\n",
        "        Forward pass for the entire sequence.\n",
        "        Args:\n",
        "            input_sequence (torch.Tensor): A sequence of token IDs. Shape: (batch_size, seq_len)\n",
        "        Returns:\n",
        "            torch.Tensor: The output logits for each step in the sequence. Shape: (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_sequence.shape\n",
        "\n",
        "        # 1. Embed the input sequence\n",
        "        embedded_input = self.embedding(input_sequence)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 2. Initialize the hidden state with zeros\n",
        "        state = torch.zeros(batch_size, self.d_model, device=input_sequence.device)\n",
        "\n",
        "        # 3. Process the sequence step-by-step\n",
        "        outputs = []\n",
        "        for i in range(seq_len):\n",
        "            # Get the input for the current time step\n",
        "            step_input = embedded_input[:, i, :]  # Shape: (batch_size, d_model)\n",
        "\n",
        "            # Pass through the recursive block\n",
        "            output, state = self.recursive_block(step_input, state)\n",
        "            outputs.append(output)\n",
        "\n",
        "        # 4. Stack the outputs and project to vocabulary size\n",
        "        # Stack along the sequence dimension\n",
        "        outputs_tensor = torch.stack(outputs, dim=1)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Final normalization and linear projection\n",
        "        normalized_outputs = self.norm_out(outputs_tensor)\n",
        "        logits = self.output_linear(normalized_outputs)\n",
        "\n",
        "        return logits\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "mJaPYCJr4MYJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw-UotGP4MYJ"
      },
      "source": [
        "## 3. Forward Pass Demonstration\n",
        "\n",
        "Let's create a dummy input sequence and pass it through our model to see the shapes of the tensors at each step. This helps verify that our implementation is working correctly.\n"
      ],
      "id": "zw-UotGP4MYJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd-96SYs4MYK",
        "outputId": "6e239a55-1128-4ee2-ceab-c226665d9f85"
      },
      "source": [
        "# Model parameters\n",
        "VOCAB_SIZE = 20\n",
        "D_MODEL = 32\n",
        "SEQ_LEN = 5\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# Instantiate the model\n",
        "model = TRM(vocab_size=VOCAB_SIZE, d_model=D_MODEL)\n",
        "print(\"Model Architecture:\")\n",
        "print(model)\n",
        "\n",
        "# Create a dummy input sequence (batch_size, seq_len)\n",
        "# These are random token IDs from our vocabulary\n",
        "dummy_input = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LEN))\n",
        "print(f\"\\nDummy Input Shape: {dummy_input.shape}\")\n",
        "print(f\"Dummy Input Tensor:\\n{dummy_input}\")\n",
        "\n",
        "# Perform a forward pass\n",
        "output_logits = model(dummy_input)\n",
        "\n",
        "print(f\"\\nOutput Logits Shape: {output_logits.shape}\")\n",
        "print(\"This shape (batch_size, seq_len, vocab_size) is what we expect!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Architecture:\n",
            "TRM(\n",
            "  (embedding): Embedding(20, 32)\n",
            "  (recursive_block): RecursiveBlock(\n",
            "    (norm_state): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm_input): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (activation): GELU(approximate='none')\n",
            "    (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  )\n",
            "  (norm_out): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "  (output_linear): Linear(in_features=32, out_features=20, bias=True)\n",
            ")\n",
            "\n",
            "Dummy Input Shape: torch.Size([1, 5])\n",
            "Dummy Input Tensor:\n",
            "tensor([[ 1,  1, 15,  1, 19]])\n",
            "\n",
            "Output Logits Shape: torch.Size([1, 5, 20])\n",
            "This shape (batch_size, seq_len, vocab_size) is what we expect!\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "Xd-96SYs4MYK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wrVorGj4MYK"
      },
      "source": [
        "## 4. A Simple Training Example\n",
        "\n",
        "To show that the model can learn, let's create a simple \"next token prediction\" task. Our goal is to train the model to predict the next number in a sequence.\n",
        "\n",
        "-   **Input:** `[1, 2, 3, 4]`\n",
        "-   **Target:** `[2, 3, 4, 5]`\n",
        "\n",
        "We'll use Cross-Entropy Loss and the Adam optimizer.\n"
      ],
      "id": "6wrVorGj4MYK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P47QgRQi4MYL",
        "outputId": "ace3afb3-8f50-43a0-8b2e-9ce2d7fb21b3"
      },
      "source": [
        "# --- Training Setup ---\n",
        "# Let's use a slightly bigger vocabulary for this task\n",
        "TRAINING_VOCAB_SIZE = 10\n",
        "TRAINING_D_MODEL = 16\n",
        "\n",
        "# Create a new model instance for training\n",
        "training_model = TRM(vocab_size=TRAINING_VOCAB_SIZE, d_model=TRAINING_D_MODEL)\n",
        "optimizer = optim.Adam(training_model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Data ---\n",
        "# Our simple sequence prediction task\n",
        "input_data = torch.tensor([[1, 2, 3, 4]])      # Shape: (1, 4)\n",
        "target_data = torch.tensor([[2, 3, 4, 5]])     # Shape: (1, 4)\n",
        "\n",
        "print(f\"Input:  {input_data[0].tolist()}\")\n",
        "print(f\"Target: {target_data[0].tolist()}\")\n",
        "\n",
        "# --- Training Loop ---\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    logits = training_model(input_data) # Shape: (batch, seq_len, vocab_size)\n",
        "\n",
        "    # Reshape for the loss function\n",
        "    # The loss function expects (N, C) where C is number of classes\n",
        "    # Logits: (1, 4, 10) -> (4, 10)\n",
        "    # Target: (1, 4) -> (4)\n",
        "    loss = criterion(logits.view(-1, TRAINING_VOCAB_SIZE), target_data.view(-1))\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# --- Inference after training ---\n",
        "print(\"\\n--- Testing after training ---\")\n",
        "with torch.no_grad():\n",
        "    test_input = torch.tensor([[1, 2, 3, 4]])\n",
        "    predictions = training_model(test_input)\n",
        "    # Get the predicted token ID by finding the max logit\n",
        "    predicted_ids = torch.argmax(predictions, dim=2)\n",
        "\n",
        "    print(f\"Input:           {test_input[0].tolist()}\")\n",
        "    print(f\"Predicted sequence: {predicted_ids[0].tolist()}\")\n",
        "    print(f\"Target sequence:    {target_data[0].tolist()}\")\n",
        "    print(\"\\nThe model has learned to predict the next number in the sequence!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  [1, 2, 3, 4]\n",
            "Target: [2, 3, 4, 5]\n",
            "Epoch [10/100], Loss: 0.3444\n",
            "Epoch [20/100], Loss: 0.0682\n",
            "Epoch [30/100], Loss: 0.0196\n",
            "Epoch [40/100], Loss: 0.0091\n",
            "Epoch [50/100], Loss: 0.0058\n",
            "Epoch [60/100], Loss: 0.0044\n",
            "Epoch [70/100], Loss: 0.0037\n",
            "Epoch [80/100], Loss: 0.0031\n",
            "Epoch [90/100], Loss: 0.0028\n",
            "Epoch [100/100], Loss: 0.0024\n",
            "\n",
            "--- Testing after training ---\n",
            "Input:           [1, 2, 3, 4]\n",
            "Predicted sequence: [2, 3, 4, 5]\n",
            "Target sequence:    [2, 3, 4, 5]\n",
            "\n",
            "The model has learned to predict the next number in the sequence!\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "P47QgRQi4MYL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKUJdfdZ4MYL"
      },
      "source": [
        "## 5. Key Differences from Transformers\n",
        "\n",
        "As summarized in the original repository, TRMs differ from Transformers in several key ways:\n",
        "\n",
        "-   **Computation:** TRMs are **recursive** and process sequences step-by-step, making their complexity linear, O(L). Transformers use **self-attention**, which is parallelizable but has quadratic complexity, O(LÂ²).\n",
        "-   **State Management:** TRMs explicitly manage a **hidden state** that evolves over time. Transformers are stateless and recompute context from scratch at each layer.\n",
        "-   **Positional Information:** TRMs inherently understand sequence order due to their sequential processing. Transformers require explicit **positional encodings** to be added to their inputs.\n"
      ],
      "id": "IKUJdfdZ4MYL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf39d8d4",
        "outputId": "e3793d75-2ffd-4d84-e594-603fc9a94ade"
      },
      "source": [
        "print(f\"Number of lines in dummy_input: {dummy_input.shape[0]}\")"
      ],
      "id": "cf39d8d4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines in dummy_input: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xn3PlLb4MYL"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook provided a brief, practical introduction to Tiny Recursive Models. We implemented the core components in PyTorch and demonstrated that even a simple TRM can learn a basic sequence prediction task.\n",
        "\n",
        "TRMs offer an interesting alternative to Transformers, particularly for applications where computational efficiency and explicit state management are important.\n"
      ],
      "id": "5xn3PlLb4MYL"
    }
  ]
}