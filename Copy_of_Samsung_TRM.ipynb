{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weagan/Tiny-Recursive-Models/blob/main/Copy_of_Samsung_TRM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV1G2k37v7FF"
      },
      "source": [
        "# Less is More: Recursive Reasoning with Tiny Networks (TRM)\n",
        "\n",
        "This notebook implements the Tiny Recursion Model (TRM) from the paper:\n",
        "\"Less is More: Recursive Reasoning with Tiny Networks\"\n",
        "\n",
        "**Key Features:**\n",
        "- 45% accuracy on ARC-AGI-1 with only 7M parameters\n",
        "- 8% accuracy on ARC-AGI-2\n",
        "- Recursive reasoning without massive models\n",
        "\n",
        "**Paper:** https://arxiv.org/abs/2510.04871\n",
        "\n",
        "**Original Code:** Based on Hierarchical Reasoning Model (HRM)\n",
        "\n",
        "**Runtime Requirements:**\n",
        "- ARC-AGI training: ~3 days on 4x H-100 GPUs\n",
        "- Sudoku-Extreme: <36 hours on 1x L40S GPU\n",
        "- Maze-Hard: <24 hours on 4x L40S GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7V7b66Vv7FJ"
      },
      "source": [
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2WUOfZjv7FK",
        "outputId": "01af8280-20f9-4105-8e3e-0bccbea55402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# Check GPU and System Info\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA version:\", torch.version.cuda)\n",
        "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_QvmCIFv7FQ",
        "outputId": "eff41406-e3d0-4660-8f32-dd2993d6f68f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Samsung-TRM'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 58 (delta 16), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (58/58), 722.67 KiB | 845.00 KiB/s, done.\n",
            "/content/Samsung-TRM\n"
          ]
        }
      ],
      "source": [
        "# Clone Repository\n",
        "!git clone https://huggingface.co/wtfmahe/Samsung-TRM\n",
        "%cd Samsung-TRM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ioTPjZbv7FR",
        "outputId": "18b96d84-aece-4502-cfca-9427d0124ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m1.4/1.8 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for adam-atan2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "✓ All dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install Dependencies\n",
        "# Note: Adjust PyTorch installation based on your CUDA version\n",
        "\n",
        "# Upgrade pip and core tools\n",
        "!pip install --upgrade pip wheel setuptools -q\n",
        "\n",
        "# Install PyTorch (CPU-only)\n",
        "!pip install --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q\n",
        "\n",
        "# Install other requirements\n",
        "!pip install -r requirements.txt -q\n",
        "\n",
        "# Install adam-atan2 optimizer\n",
        "!pip install --no-cache-dir --no-build-isolation adam-atan2 -q\n",
        "\n",
        "print(\"✓ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GvD3Y-53v7FR"
      },
      "outputs": [],
      "source": [
        "# Login to Weights & Biases (Optional)\n",
        "# Skip this cell if you don't want to use W&B\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Option 1: Login interactively\n",
        "#wandb.login()\n",
        "\n",
        "# Option 2: Login with API key (uncomment and add your key)\n",
        "# wandb.login(key=\"YOUR_WANDB_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x63uIJU1v7FT"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Any, Sequence, List\n",
        "from dataclasses import dataclass\n",
        "import os\n",
        "import math\n",
        "import yaml\n",
        "import shutil\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import tqdm\n",
        "import wandb\n",
        "import coolname\n",
        "# import hydra # No longer needed for hardcoding\n",
        "import pydantic\n",
        "from omegaconf import DictConfig\n",
        "# from adam_atan2 import AdamATan2 # Removed problematic import\n",
        "import torch.optim as optim # Added standard optimizer\n",
        "\n",
        "from puzzle_dataset import PuzzleDataset, PuzzleDatasetConfig, PuzzleDatasetMetadata\n",
        "from utils.functions import load_model_class, get_model_source_path\n",
        "from models.sparse_embedding import CastedSparseEmbeddingSignSGD_Distributed\n",
        "from models.ema import EMAHelper\n",
        "\n",
        "\n",
        "class LossConfig(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra='allow')\n",
        "    name: str\n",
        "\n",
        "\n",
        "class ArchConfig(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra='allow')\n",
        "    name: str\n",
        "    loss: LossConfig\n",
        "    puzzle_emb_ndim: int = 0 # Default value, ensure it's present if not in original config\n",
        "    mlp_t: Optional[bool] = None # Added for specific arch config\n",
        "    pos_encodings: Optional[str] = None # Added for specific arch config\n",
        "    L_layers: Optional[int] = None\n",
        "    H_cycles: Optional[int] = None\n",
        "    L_cycles: Optional[int] = None\n",
        "\n",
        "\n",
        "class EvaluatorConfig(pydantic.BaseModel):\n",
        "    model_config = pydantic.ConfigDict(extra=\"allow\")\n",
        "    name: str\n",
        "\n",
        "\n",
        "class PretrainConfig(pydantic.BaseModel):\n",
        "    # Config\n",
        "    arch: ArchConfig\n",
        "    # Data\n",
        "    data_paths: List[str]\n",
        "    data_paths_test: List[str] = []\n",
        "    # Evaluators\n",
        "    evaluators: List[EvaluatorConfig] = []\n",
        "\n",
        "    # Hyperparams\n",
        "    global_batch_size: int = 1 # Adjusted for single CPU run, usually this is larger\n",
        "    epochs: int\n",
        "\n",
        "    lr: float\n",
        "    lr_min_ratio: float = 0.1 # Default value, ensure it's present if not in original config\n",
        "    lr_warmup_steps: int = 1000 # Default value, ensure it's present if not in original config\n",
        "\n",
        "    weight_decay: float\n",
        "    beta1: float = 0.9 # Default value, ensure it's present if not in original config\n",
        "    beta2: float = 0.95 # Default value, ensure it's present if not in original config\n",
        "\n",
        "    # Puzzle embedding\n",
        "    puzzle_emb_lr: float\n",
        "    puzzle_emb_weight_decay: float\n",
        "\n",
        "    # Names\n",
        "    project_name: Optional[str] = None\n",
        "    run_name: Optional[str] = None\n",
        "    load_checkpoint: Optional[str] = None\n",
        "    checkpoint_path: Optional[str] = None\n",
        "\n",
        "    # Extras\n",
        "    seed: int = 0\n",
        "    checkpoint_every_eval: bool = False\n",
        "    eval_interval: Optional[int] = None\n",
        "    min_eval_interval: Optional[int] = 0 # when to start eval\n",
        "    eval_save_outputs: List[str] = []\n",
        "\n",
        "    ema: bool = False # use Exponential-Moving-Average\n",
        "    ema_rate: float = 0.999 # EMA-rate\n",
        "    freeze_weights: bool = False # If True, freeze weights and only learn the embeddings\n",
        "\n",
        "@dataclass\n",
        "class TrainState:\n",
        "    model: nn.Module\n",
        "    optimizers: Sequence[torch.optim.Optimizer]\n",
        "    optimizer_lrs: Sequence[float]\n",
        "    carry: Any\n",
        "\n",
        "    step: int\n",
        "    total_steps: int\n",
        "\n",
        "\n",
        "def create_dataloader(config: PretrainConfig, split: str, rank: int, world_size: int, **kwargs):\n",
        "    dataset = PuzzleDataset(PuzzleDatasetConfig(\n",
        "        seed=config.seed,\n",
        "        dataset_paths=config.data_paths_test if len(config.data_paths_test)>0 and split==\"test\" else config.data_paths,\n",
        "        rank=rank,\n",
        "        num_replicas=world_size,\n",
        "        **kwargs\n",
        "    ), split=split)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=None,\n",
        "        num_workers=1,\n",
        "        prefetch_factor=8,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "    return dataloader, dataset.metadata\n",
        "\n",
        "\n",
        "def create_model(config: PretrainConfig, train_metadata: PuzzleDatasetMetadata, rank: int, world_size: int):\n",
        "    model_cfg = dict(\n",
        "        **config.arch.model_dump(exclude_unset=True), # Use model_dump and exclude_unset to handle optional fields\n",
        "        batch_size=config.global_batch_size // world_size,\n",
        "        vocab_size=train_metadata.vocab_size,\n",
        "        seq_len=train_metadata.seq_len,\n",
        "        num_puzzle_identifiers=train_metadata.num_puzzle_identifiers,\n",
        "        causal=False  # Non-autoregressive\n",
        "    )\n",
        "\n",
        "    # Instantiate model with loss head\n",
        "    model_cls = load_model_class(config.arch.name)\n",
        "    loss_head_cls = load_model_class(config.arch.loss.name)\n",
        "\n",
        "    # Changed device to cpu\n",
        "    with torch.device(\"cpu\"):\n",
        "        model: nn.Module = model_cls(model_cfg)\n",
        "        print(model)\n",
        "        model = loss_head_cls(model, **config.arch.loss.model_dump(exclude_unset=True))  # type: ignore\n",
        "        if \"DISABLE_COMPILE\" not in os.environ:\n",
        "            # torch.compile not available or recommended for CPU-only in all versions/setups\n",
        "            # model = torch.compile(model)  # type: ignore\n",
        "            pass # disable torch.compile for CPU\n",
        "\n",
        "        # Load checkpoint\n",
        "        if rank == 0:\n",
        "            load_checkpoint(model, config)\n",
        "\n",
        "        # Broadcast parameters from rank 0\n",
        "        if world_size > 1:\n",
        "            with torch.no_grad():\n",
        "                for param in list(model.parameters()) + list(model.buffers()):\n",
        "                    dist.broadcast(param, src=0)\n",
        "\n",
        "    # Optimizers and lr\n",
        "    if config.arch.puzzle_emb_ndim == 0:\n",
        "        optimizers = [\n",
        "            optim.Adam(\n",
        "                model.parameters(),\n",
        "                lr=0,  # Needs to be set by scheduler\n",
        "                weight_decay=config.weight_decay,\n",
        "                betas=(config.beta1, config.beta2)\n",
        "            )\n",
        "        ]\n",
        "        optimizer_lrs = [\n",
        "            config.lr\n",
        "        ]\n",
        "    elif config.freeze_weights:\n",
        "        optimizers = [\n",
        "            CastedSparseEmbeddingSignSGD_Distributed(\n",
        "                model.model.puzzle_emb.buffers(),  # type: ignore\n",
        "                lr=0,  # Needs to be set by scheduler\n",
        "                weight_decay=config.puzzle_emb_weight_decay,\n",
        "                world_size=world_size\n",
        "            )\n",
        "        ]\n",
        "        optimizer_lrs = [\n",
        "            config.puzzle_emb_lr\n",
        "        ]\n",
        "    else:\n",
        "        optimizers = [\n",
        "            CastedSparseEmbeddingSignSGD_Distributed(\n",
        "                model.model.puzzle_emb.buffers(),  # type: ignore\n",
        "                lr=0,  # Needs to be set by scheduler\n",
        "                weight_decay=config.puzzle_emb_weight_decay,\n",
        "                world_size=world_size\n",
        "            ),\n",
        "            optim.Adam(\n",
        "                model.parameters(),\n",
        "                lr=0,  # Needs to be set by scheduler\n",
        "                weight_decay=config.weight_decay,\n",
        "                betas=(config.beta1, config.beta2)\n",
        "            )\n",
        "        ]\n",
        "        optimizer_lrs = [\n",
        "            config.puzzle_emb_lr,\n",
        "            config.lr\n",
        "        ]\n",
        "\n",
        "    return model, optimizers, optimizer_lrs\n",
        "\n",
        "def mix_weights_direct(device, alpha, net, nets):\n",
        "    sd = []\n",
        "    for i in range(len(nets)):\n",
        "        sd += [nets[i].state_dict()]\n",
        "    sd_alpha = {}\n",
        "    for k in sd[0].keys():\n",
        "        comb_net = alpha[0]*sd[0][k].to(device)\n",
        "        for i in range(1,len(nets)):\n",
        "            comb_net += alpha[i]*sd[i][k].to(device)\n",
        "        sd_alpha[k] =  comb_net\n",
        "    net.load_state_dict(sd_alpha)\n",
        "    return net\n",
        "\n",
        "def cosine_schedule_with_warmup_lr_lambda(\n",
        "    current_step: int, *, base_lr: float, num_warmup_steps: int, num_training_steps: int, min_ratio: float = 0.0, num_cycles: float = 0.5\n",
        "):\n",
        "    if current_step < num_warmup_steps:\n",
        "        return base_lr * float(current_step) / float(max(1, num_warmup_steps))\n",
        "\n",
        "    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "    return base_lr * (min_ratio + max(0.0, (1 - min_ratio) * 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))))\n",
        "\n",
        "\n",
        "def init_train_state(config: PretrainConfig, train_metadata: PuzzleDatasetMetadata, rank: int, world_size: int):\n",
        "    # Estimated total training steps\n",
        "    total_steps = int(config.epochs * train_metadata.total_groups * train_metadata.mean_puzzle_examples / config.global_batch_size)\n",
        "\n",
        "    # Model\n",
        "    model, optimizers, optimizer_lrs = create_model(config, train_metadata, rank=rank, world_size=world_size)\n",
        "\n",
        "    return TrainState(\n",
        "        step=0,\n",
        "        total_steps=total_steps,\n",
        "\n",
        "        model=model,\n",
        "        optimizers=optimizers,\n",
        "        optimizer_lrs=optimizer_lrs,\n",
        "        carry=None\n",
        "    )\n",
        "\n",
        "\n",
        "def save_train_state(config: PretrainConfig, train_state: TrainState):\n",
        "    # FIXME: Only saved model.\n",
        "    if config.checkpoint_path is None:\n",
        "        return\n",
        "\n",
        "    os.makedirs(config.checkpoint_path, exist_ok=True)\n",
        "    torch.save(train_state.model.state_dict(), os.path.join(config.checkpoint_path, f\"step_{train_state.step}\"))\n",
        "\n",
        "\n",
        "def load_checkpoint(model: nn.Module, config: PretrainConfig):\n",
        "    if config.load_checkpoint is not None:\n",
        "        print(f\"Loading checkpoint {config.load_checkpoint}\")\n",
        "\n",
        "        # Changed map_location to cpu\n",
        "        state_dict = torch.load(config.load_checkpoint, map_location=\"cpu\")\n",
        "\n",
        "        # Resize and reset puzzle emb if needed\n",
        "        puzzle_emb_name = \"_orig_mod.model.inner.puzzle_emb.weights\"\n",
        "        expected_shape: torch.Size = model.model.puzzle_emb.weights.shape  # type: ignore\n",
        "        if puzzle_emb_name in state_dict:\n",
        "            puzzle_emb = state_dict[puzzle_emb_name]\n",
        "            if puzzle_emb.shape != expected_shape:\n",
        "                print(f\"Resetting puzzle embedding as shape is different. Found {puzzle_emb.shape}, Expected {expected_shape}\")\n",
        "                # Re-initialize using mean\n",
        "                state_dict[puzzle_emb_name] = (\n",
        "                    torch.mean(puzzle_emb, dim=0, keepdim=True).expand(expected_shape).contiguous()\n",
        "                )\n",
        "        model.load_state_dict(state_dict, assign=True)\n",
        "\n",
        "\n",
        "def compute_lr(base_lr: float, config: PretrainConfig, train_state: TrainState):\n",
        "    return cosine_schedule_with_warmup_lr_lambda(\n",
        "        current_step=train_state.step,\n",
        "        base_lr=base_lr,\n",
        "        num_warmup_steps=round(config.lr_warmup_steps),\n",
        "        num_training_steps=train_state.total_steps,\n",
        "        min_ratio=config.lr_min_ratio\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def create_evaluators(config: PretrainConfig, eval_metadata: PuzzleDatasetMetadata) -> List[Any]:\n",
        "    data_paths =config.data_paths_test if len(config.data_paths_test)>0 else config.data_paths\n",
        "    # Initialize evaluators\n",
        "    evaluators = []\n",
        "    for cfg in config.evaluators:\n",
        "        for data_path in data_paths:\n",
        "            cls = load_model_class(cfg.name, \"evaluators.\")(\n",
        "                data_path=data_path, eval_metadata=eval_metadata, **cfg.__pydantic_extra__\n",
        "            )  # type: ignore\n",
        "            evaluators.append(cls)\n",
        "\n",
        "    return evaluators\n",
        "\n",
        "def train_batch(config: PretrainConfig, train_state: TrainState, batch: Any, global_batch_size: int, rank: int, world_size: int):\n",
        "    train_state.step += 1\n",
        "    if train_state.step > train_state.total_steps:  # At most train_total_steps\n",
        "        return\n",
        "\n",
        "    # To device (changed to cpu)\n",
        "    batch = {k: v.to(\"cpu\") for k, v in batch.items()}\n",
        "\n",
        "    # Init carry if it is None\n",
        "    if train_state.carry is None:\n",
        "        with torch.device(\"cpu\"):\n",
        "            train_state.carry = train_state.model.initial_carry(batch)  # type: ignore\n",
        "\n",
        "    # Forward\n",
        "    train_state.carry, loss, metrics, _, _ = train_state.model(carry=train_state.carry, batch=batch, return_keys=[])\n",
        "\n",
        "    ((1 / global_batch_size) * loss).backward()\n",
        "\n",
        "    # Allreduce (not applicable for world_size=1 CPU run)\n",
        "    if world_size > 1:\n",
        "        for param in train_state.model.parameters():\n",
        "            if param.grad is not None:\n",
        "                dist.all_reduce(param.grad)\n",
        "\n",
        "    # Apply optimizer\n",
        "    lr_this_step = None\n",
        "    for optim, base_lr in zip(train_state.optimizers, train_state.optimizer_lrs):\n",
        "        lr_this_step = compute_lr(base_lr, config, train_state)\n",
        "\n",
        "        for param_group in optim.param_groups:\n",
        "            param_group['lr'] = lr_this_step\n",
        "\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "    # Reduce metrics (simplified for world_size=1 CPU run)\n",
        "    if len(metrics):\n",
        "        assert not any(v.requires_grad for v in metrics.values())\n",
        "\n",
        "        metric_keys = list(sorted(metrics.keys()))  # Sort keys to guarantee all processes use the same order.\n",
        "        # Reduce and reconstruct\n",
        "        metric_values = torch.stack([metrics[k] for k in metric_keys])\n",
        "        # if world_size > 1: # Not needed for single GPU/CPU\n",
        "        #     dist.reduce(metric_values, dst=0)\n",
        "\n",
        "        # if rank == 0: # Always rank 0 for single process\n",
        "        metric_values = metric_values.cpu().numpy()\n",
        "        reduced_metrics = {k: metric_values[i] for i, k in enumerate(metric_keys)}\n",
        "\n",
        "        # Postprocess\n",
        "        count = max(reduced_metrics[\"count\"], 1)  # Avoid NaNs\n",
        "        reduced_metrics = {f\"train/{k}\": v / (global_batch_size if k.endswith(\"loss\") else count) for k, v in reduced_metrics.items()}\n",
        "\n",
        "        reduced_metrics[\"train/lr\"] = lr_this_step\n",
        "        return reduced_metrics\n",
        "    return {}\n",
        "\n",
        "def evaluate(\n",
        "    config: PretrainConfig,\n",
        "    train_state: TrainState,\n",
        "    eval_loader: torch.utils.data.DataLoader,\n",
        "    eval_metadata: PuzzleDatasetMetadata,\n",
        "    evaluators: List[Any],\n",
        "    rank: int,\n",
        "    world_size: int,\n",
        "    cpu_group: Optional[dist.ProcessGroup],\n",
        "):\n",
        "    reduced_metrics = None\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        return_keys = set(config.eval_save_outputs)\n",
        "        for evaluator in evaluators:\n",
        "            evaluator.begin_eval()\n",
        "            return_keys.update(evaluator.required_outputs)\n",
        "\n",
        "        # Run evaluation\n",
        "        set_ids = {k: idx for idx, k in enumerate(eval_metadata.sets)}\n",
        "\n",
        "        save_preds = {}\n",
        "\n",
        "        metric_keys = []\n",
        "        metric_values = None\n",
        "\n",
        "        carry = None\n",
        "        processed_batches = 0\n",
        "\n",
        "        for set_name, batch, global_batch_size in eval_loader:\n",
        "            processed_batches += 1\n",
        "            if rank == 0:\n",
        "                print(f\"Processing batch {processed_batches}: {set_name}\")\n",
        "\n",
        "            # To device (changed to cpu)\n",
        "            batch = {k: v.to(\"cpu\") for k, v in batch.items()}\n",
        "            with torch.device(\"cpu\"):\n",
        "                carry = train_state.model.initial_carry(batch)  # type: ignore\n",
        "\n",
        "            # Forward\n",
        "            inference_steps = 0\n",
        "            while True:\n",
        "                carry, loss, metrics, preds, all_finish = train_state.model(\n",
        "                    carry=carry, batch=batch, return_keys=return_keys\n",
        "                )\n",
        "                inference_steps += 1\n",
        "\n",
        "                if all_finish:\n",
        "                    break\n",
        "\n",
        "            if rank == 0:\n",
        "                print(f\"  Completed inference in {inference_steps} steps\")\n",
        "\n",
        "            for collection in (batch, preds):\n",
        "                for k, v in collection.items():\n",
        "                    if k in config.eval_save_outputs:\n",
        "                        save_preds.setdefault(k, [])\n",
        "                        save_preds[k].append(v.cpu())  # Move to CPU for saving GPU memory\n",
        "\n",
        "            for evaluator in evaluators:\n",
        "                evaluator.update_batch(batch, preds)\n",
        "\n",
        "            del carry, loss, preds, batch, all_finish\n",
        "\n",
        "            # Aggregate metrics\n",
        "            set_id = set_ids[set_name]\n",
        "\n",
        "            if metric_values is None:\n",
        "                metric_keys = list(\n",
        "                    sorted(metrics.keys())\n",
        "                )  # Sort keys to guarantee all processes use the same order.\n",
        "                metric_values = torch.zeros(\n",
        "                    (len(set_ids), len(metrics.values())), dtype=torch.float32, device=\"cpu\"\n",
        "                )\n",
        "\n",
        "            metric_values[set_id] += torch.stack([metrics[k] for k in metric_keys])\n",
        "\n",
        "            del metrics\n",
        "\n",
        "        # concatenate save preds\n",
        "        save_preds = {k: torch.cat(v, dim=0) for k, v in save_preds.items()}\n",
        "\n",
        "        # Save preds\n",
        "        if config.checkpoint_path is not None and len(save_preds):\n",
        "            # Each rank save predictions independently\n",
        "            os.makedirs(os.path.dirname(config.checkpoint_path), exist_ok=True)\n",
        "            torch.save(\n",
        "                save_preds, os.path.join(config.checkpoint_path, f\"step_{train_state.step}_all_preds.{rank}\")\n",
        "            )\n",
        "\n",
        "        del save_preds\n",
        "\n",
        "        # Reduce to rank 0 (not applicable for world_size=1 CPU run)\n",
        "        if metric_values is not None:\n",
        "            # if world_size > 1:\n",
        "            #     dist.reduce(metric_values, dst=0)\n",
        "\n",
        "            # if rank == 0:\n",
        "            reduced_metrics = metric_values.cpu().numpy()\n",
        "            reduced_metrics = {\n",
        "                set_name: {\n",
        "                    metric_name: reduced_metrics[set_id, metric_id]\n",
        "                    for metric_id, metric_name in enumerate(metric_keys)\n",
        "                }\n",
        "                for set_id, set_name in enumerate(set_ids)\n",
        "            }\n",
        "\n",
        "            # Postprocess\n",
        "            for set_name, m in reduced_metrics.items():\n",
        "                count = m.pop(\"count\")\n",
        "                reduced_metrics[set_name] = {k: v / count for k, v in m.items()}\n",
        "\n",
        "        # Run evaluators\n",
        "        if rank == 0:\n",
        "            print(f\"\\nRunning {len(evaluators)} evaluator(s)...\")\n",
        "\n",
        "        for i, evaluator in enumerate(evaluators):\n",
        "            if rank == 0:\n",
        "                print(f\"Running evaluator {i+1}/{len(evaluators)}: {evaluator.__class__.__name__}\")\n",
        "\n",
        "            # Path for saving\n",
        "            evaluator_save_path = None\n",
        "            if config.checkpoint_path is not None:\n",
        "                evaluator_save_path = os.path.join(\n",
        "                    config.checkpoint_path,\n",
        "                    f\"evaluator_{evaluator.__class__.__name__}_step_{train_state.step}\",\n",
        "                )\n",
        "                os.makedirs(evaluator_save_path, exist_ok=True)\n",
        "\n",
        "            # Run and log\n",
        "            metrics = evaluator.result(evaluator_save_path, rank=rank, world_size=world_size, group=cpu_group)\n",
        "            if rank == 0 and metrics is not None:\n",
        "                if reduced_metrics is None:\n",
        "                    reduced_metrics = {}\n",
        "\n",
        "                reduced_metrics.update(metrics)\n",
        "                print(f\"  Completed {evaluator.__class__.__name__}\")\n",
        "\n",
        "        if rank == 0:\n",
        "            print(\"All evaluators completed!\")\n",
        "\n",
        "    return reduced_metrics\n",
        "\n",
        "def save_code_and_config(config: PretrainConfig):\n",
        "    if config.checkpoint_path is None or wandb.run is None:\n",
        "        return\n",
        "\n",
        "    os.makedirs(config.checkpoint_path, exist_ok=True)\n",
        "\n",
        "    # Copy code\n",
        "    code_list = [\n",
        "        get_model_source_path(config.arch.name),\n",
        "        get_model_source_path(config.arch.loss.name)\n",
        "    ]\n",
        "    for code_file in code_list:\n",
        "        if code_file is not None:\n",
        "            code_name = os.path.basename(code_file)\n",
        "\n",
        "            shutil.copy(code_file, os.path.join(config.checkpoint_path, code_name))\n",
        "\n",
        "    # Dump config as yaml\n",
        "    config_file = os.path.join(config.checkpoint_path, \"all_config.yaml\")\n",
        "    with open(config_file, \"wt\") as f:\n",
        "        yaml.dump(config.model_dump(), f)\n",
        "\n",
        "    # Log code\n",
        "    wandb.run.log_code(config.checkpoint_path)\n",
        "\n",
        "\n",
        "def load_synced_config(hydra_config: DictConfig, rank: int, world_size: int) -> PretrainConfig:\n",
        "    objects = [None]\n",
        "    if rank == 0:\n",
        "        config = PretrainConfig(**hydra_config)  # type: ignore\n",
        "\n",
        "        # Naming\n",
        "        if config.project_name is None:\n",
        "            config.project_name = f\"{os.path.basename(config.data_paths[0]).capitalize()}-ACT-torch\"\n",
        "        if config.run_name is None:\n",
        "            config.run_name = f\"{config.arch.name.split('@')[-1]} {coolname.generate_slug(2)}\"\n",
        "        if config.checkpoint_path is None:\n",
        "            config.checkpoint_path = os.path.join(\"checkpoints\", config.project_name, config.run_name)\n",
        "\n",
        "        objects = [config]\n",
        "\n",
        "    if world_size > 1:\n",
        "        dist.broadcast_object_list(objects, src=0)\n",
        "\n",
        "    return objects[0]  # type: ignore\n",
        "\n",
        "\n",
        "# @hydra.main(config_path=\"config\", config_name=\"cfg_pretrain\", version_base=None) # Removed Hydra decorator\n",
        "def launch():\n",
        "    # Hardcoded values for Maze-Hard task, single CPU run\n",
        "    RANK = 0\n",
        "    WORLD_SIZE = 1\n",
        "    CPU_PROCESS_GROUP = None\n",
        "\n",
        "    # Hardcode PretrainConfig directly\n",
        "    config = PretrainConfig(\n",
        "        arch=ArchConfig(\n",
        "            name='common@TRM', # Modified to 'common@TRM'\n",
        "            loss=LossConfig(name='losses@CrossEntropyLoss'), # Modified to 'losses@CrossEntropyLoss'\n",
        "            L_layers=2,\n",
        "            H_cycles=3,\n",
        "            L_cycles=4\n",
        "        ),\n",
        "        data_paths=[\"data/maze-30x30-hard-1k\"],\n",
        "        evaluators=[], # Empty list as specified\n",
        "        epochs=50000,\n",
        "        eval_interval=5000,\n",
        "        lr=1e-4,\n",
        "        puzzle_emb_lr=1e-4,\n",
        "        weight_decay=1.0,\n",
        "        puzzle_emb_weight_decay=1.0,\n",
        "        run_name=\"pretrain_att_maze30x30_hardcoded\", # Hardcoded run name\n",
        "        ema=True,\n",
        "        # Add other default values if necessary based on cfg_pretrain.yaml or common defaults\n",
        "        global_batch_size=1, # Default or set as appropriate for CPU/small batch\n",
        "        lr_min_ratio=0.1,\n",
        "        lr_warmup_steps=1000,\n",
        "        beta1=0.9,\n",
        "        beta2=0.95\n",
        "    )\n",
        "\n",
        "    # Naming - adapted for hardcoded config\n",
        "    if config.project_name is None:\n",
        "        config.project_name = f\"{os.path.basename(config.data_paths[0]).capitalize()}-ACT-torch\"\n",
        "    if config.checkpoint_path is None:\n",
        "        config.checkpoint_path = os.path.join(\"checkpoints\", config.project_name, config.run_name)\n",
        "\n",
        "\n",
        "    # Seed RNGs to ensure consistency\n",
        "    torch.random.manual_seed(config.seed + RANK)\n",
        "\n",
        "    # Dataset\n",
        "    train_epochs_per_iter = config.eval_interval if config.eval_interval is not None else config.epochs\n",
        "    total_iters = config.epochs // train_epochs_per_iter\n",
        "\n",
        "    assert config.epochs % train_epochs_per_iter == 0, \"Eval interval must be a divisor of total epochs.\"\n",
        "\n",
        "    train_loader, train_metadata = create_dataloader(config, \"train\", test_set_mode=False, epochs_per_iter=train_epochs_per_iter, global_batch_size=config.global_batch_size, rank=RANK, world_size=WORLD_SIZE)\n",
        "    try:\n",
        "        eval_loader,  eval_metadata  = create_dataloader(config, \"test\", test_set_mode=True, epochs_per_iter=1, global_batch_size=config.global_batch_size, rank=RANK, world_size=WORLD_SIZE)\n",
        "    except:\n",
        "        print(\"NO EVAL DATA FOUND\")\n",
        "        eval_loader = eval_metadata = None\n",
        "\n",
        "    try:\n",
        "        evaluators = create_evaluators(config, eval_metadata)\n",
        "    except:\n",
        "        print(\"No evaluator found\")\n",
        "        evaluators = []\n",
        "\n",
        "    # Train state\n",
        "    train_state = init_train_state(config, train_metadata, rank=RANK, world_size=WORLD_SIZE)\n",
        "\n",
        "    # Progress bar and logger\n",
        "    progress_bar = None\n",
        "    ema_helper = None\n",
        "    if RANK == 0:\n",
        "        progress_bar = tqdm.tqdm(total=train_state.total_steps)\n",
        "        wandb.init(project=config.project_name, name=config.run_name, config=config.model_dump(), settings=wandb.Settings(_disable_stats=True))  # type: ignore\n",
        "        wandb.log({\"num_params\": sum(x.numel() for x in train_state.model.parameters())}, step=0)\n",
        "        save_code_and_config(config)\n",
        "    if config.ema:\n",
        "        print('Setup EMA')\n",
        "        ema_helper = EMAHelper(mu=config.ema_rate)\n",
        "        ema_helper.register(train_state.model)\n",
        "\n",
        "    # Training Loop\n",
        "    for _iter_id in range(total_iters):\n",
        "        print (f\"[Rank {RANK}, World Size {WORLD_SIZE}]: Epoch {_iter_id * train_epochs_per_iter}\")\n",
        "\n",
        "        ############ Train Iter\n",
        "        if RANK == 0:\n",
        "            print(\"TRAIN\")\n",
        "        train_state.model.train()\n",
        "        for set_name, batch, global_batch_size in train_loader:\n",
        "            metrics = train_batch(config, train_state, batch, global_batch_size, rank=RANK, world_size=WORLD_SIZE)\n",
        "\n",
        "            if RANK == 0 and metrics is not None:\n",
        "                wandb.log(metrics, step=train_state.step)\n",
        "                progress_bar.update(train_state.step - progress_bar.n)  # type: ignore\n",
        "            if config.ema:\n",
        "                ema_helper.update(train_state.model)\n",
        "\n",
        "        if _iter_id >= config.min_eval_interval:\n",
        "            ############ Evaluation\n",
        "            if RANK == 0:\n",
        "                print(\"EVALUATE\")\n",
        "            if config.ema:\n",
        "                print(\"SWITCH TO EMA\")\n",
        "                train_state_eval = copy.deepcopy(train_state)\n",
        "                train_state_eval.model = ema_helper.ema_copy(train_state_eval.model)\n",
        "            else:\n",
        "                train_state_eval = train_state\n",
        "            train_state_eval.model.eval()\n",
        "            metrics = evaluate(config,\n",
        "                train_state_eval,\n",
        "                eval_loader,\n",
        "                eval_metadata,\n",
        "                evaluators,\n",
        "                rank=RANK,\n",
        "                world_size=WORLD_SIZE,\n",
        "                cpu_group=CPU_PROCESS_GROUP)\n",
        "\n",
        "            if RANK == 0 and metrics is not None:\n",
        "                wandb.log(metrics, step=train_state.step)\n",
        "\n",
        "            ############ Checkpointing\n",
        "            if RANK == 0 and (config.checkpoint_every_eval or (_iter_id == total_iters - 1)):\n",
        "                save_train_state(config, train_state_eval)\n",
        "\n",
        "            if config.ema:\n",
        "                del train_state_eval\n",
        "\n",
        "    # finalize\n",
        "    if dist.is_initialized():\n",
        "        dist.destroy_process_group()\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "CxyruDz2wUpe",
        "outputId": "85c9a790-e56e-4467-90df-0d5f8f562210"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1601373357.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m     \u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1601373357.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;31m# Train state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0mtrain_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_train_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWORLD_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;31m# Progress bar and logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1601373357.py\u001b[0m in \u001b[0;36minit_train_state\u001b[0;34m(config, train_metadata, rank, world_size)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_lrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     return TrainState(\n",
            "\u001b[0;32m/tmp/ipython-input-1601373357.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(config, train_metadata, rank, world_size)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# Instantiate model with loss head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mmodel_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mloss_head_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Samsung-TRM/utils/functions.py\u001b[0m in \u001b[0;36mload_model_class\u001b[0;34m(identifier, prefix)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"models.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Import the module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZelSmgeqv7FT"
      },
      "outputs": [],
      "source": [
        "# Download ARC-AGI Dataset\n",
        "# You'll need Kaggle API credentials configured\n",
        "\n",
        "# Create kaggle directory\n",
        "!mkdir -p kaggle/combined\n",
        "\n",
        "# Download ARC-AGI dataset\n",
        "# Note: You need to accept competition rules and have kaggle.json configured\n",
        "!kaggle competitions download -c arc-prize-2024\n",
        "!unzip -q arc-prize-2024.zip -d kaggle/combined/\n",
        "\n",
        "print(\"✓ ARC-AGI dataset downloaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiBCwR92v7FU"
      },
      "outputs": [],
      "source": [
        "# Build ARC-AGI-1 Dataset\n",
        "# This creates augmented versions of the data\n",
        "\n",
        "!python -m dataset.build_arc_dataset \\\n",
        "  --input-file-prefix kaggle/combined/arc-agi \\\n",
        "  --output-dir data/arc1concept-aug-1000 \\\n",
        "  --subsets training evaluation concept \\\n",
        "  --test-set-name evaluation\n",
        "\n",
        "print(\"✓ ARC-AGI-1 dataset prepared!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvIC9tCVv7FU"
      },
      "outputs": [],
      "source": [
        "# Build ARC-AGI-2 Dataset\n",
        "# Note: Cannot train on both ARC-AGI-1 and ARC-AGI-2 together\n",
        "\n",
        "!python -m dataset.build_arc_dataset \\\n",
        "  --input-file-prefix kaggle/combined/arc-agi \\\n",
        "  --output-dir data/arc2concept-aug-1000 \\\n",
        "  --subsets training2 evaluation2 concept \\\n",
        "  --test-set-name evaluation2\n",
        "\n",
        "print(\"✓ ARC-AGI-2 dataset prepared!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIqfcBOdv7FW"
      },
      "outputs": [],
      "source": [
        "# Build Sudoku-Extreme Dataset\n",
        "# Generate with 1000 examples and 1000 augmentations\n",
        "\n",
        "!python dataset/build_sudoku_dataset.py \\\n",
        "  --output-dir data/sudoku-extreme-1k-aug-1000 \\\n",
        "  --subsample-size 1000 \\\n",
        "  --num-aug 1000\n",
        "\n",
        "print(\"✓ Sudoku-Extreme dataset prepared!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YunjPEA-v7FW",
        "outputId": "14ad3b47-3376-4241-fed2-c763ba6cf029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rtrain.csv: 0.00B [00:00, ?B/s]\rtrain.csv: 1.81MB [00:00, 54.8MB/s]\n",
            "100% 1000/1000 [00:00<00:00, 762462.10it/s]\n",
            "test.csv: 1.81MB [00:00, 109MB/s]\n",
            "100% 1000/1000 [00:00<00:00, 1007761.65it/s]\n",
            "✓ Maze-Hard dataset prepared!\n"
          ]
        }
      ],
      "source": [
        "# Build Maze-Hard Dataset\n",
        "# Generate 1000 examples with 8 augmentations\n",
        "\n",
        "!python dataset/build_maze_dataset.py\n",
        "\n",
        "print(\"✓ Maze-Hard dataset prepared!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXv00Hbjv7FW"
      },
      "source": [
        "## Training Experiments\n",
        "\n",
        "### ARC-AGI Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CGeil05v7FX"
      },
      "outputs": [],
      "source": [
        "# Train on ARC-AGI-1 (Multi-GPU)\n",
        "# Requires 4 H-100 GPUs, runs for ~3 days\n",
        "\n",
        "run_name = \"pretrain_att_arc1concept_4\"\n",
        "\n",
        "!torchrun --nproc-per-node 4 \\\n",
        "  --rdzv_backend=c10d \\\n",
        "  --rdzv_endpoint=localhost:0 \\\n",
        "  --nnodes=1 \\\n",
        "  pretrain.py \\\n",
        "  arch=trm \\\n",
        "  data_paths=\"[data/arc1concept-aug-1000]\" \\\n",
        "  arch.L_layers=2 \\\n",
        "  arch.H_cycles=3 \\\n",
        "  arch.L_cycles=4 \\\n",
        "  +run_name={run_name} \\\n",
        "  ema=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dzFU7asv7FY"
      },
      "outputs": [],
      "source": [
        "# Train on ARC-AGI-1 (Single GPU - for Colab)\n",
        "# Modified for Colab constraints\n",
        "\n",
        "run_name = \"pretrain_att_arc1concept_1gpu\"\n",
        "\n",
        "!python pretrain.py \\\n",
        "  arch=trm \\\n",
        "  data_paths=\"[data/arc1concept-aug-1000]\" \\\n",
        "  arch.L_layers=2 \\\n",
        "  arch.H_cycles=3 \\\n",
        "  arch.L_cycles=4 \\\n",
        "  +run_name={run_name} \\\n",
        "  ema=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30tuZR-sv7FY"
      },
      "outputs": [],
      "source": [
        "# Train on ARC-AGI-2 (Multi-GPU)\n",
        "# Requires 4 H-100 GPUs, runs for ~3 days\n",
        "\n",
        "run_name = \"pretrain_att_arc2concept_4\"\n",
        "\n",
        "!torchrun --nproc-per-node 4 \\\n",
        "  --rdzv_backend=c10d \\\n",
        "  --rdzv_endpoint=localhost:0 \\\n",
        "  --nnodes=1 \\\n",
        "  pretrain.py \\\n",
        "  arch=trm \\\n",
        "  data_paths=\"[data/arc2concept-aug-1000]\" \\\n",
        "  arch.L_layers=2 \\\n",
        "  arch.H_cycles=3 \\\n",
        "  arch.L_cycles=4 \\\n",
        "  +run_name={run_name} \\\n",
        "  ema=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1GNujpFv7FZ"
      },
      "source": [
        "### Sudoku-Extreme Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC2CKWXMv7Fa"
      },
      "outputs": [],
      "source": [
        "# Train on Sudoku-Extreme (MLP version)\n",
        "# Runtime: <36 hours on 1 L40S GPU\n",
        "\n",
        "run_name = \"pretrain_mlp_t_sudoku\"\n",
        "\n",
        "!python pretrain.py \\\n",
        "  arch=trm \\\n",
        "  data_paths=\"[data/sudoku-extreme-1k-aug-1000]\" \\\n",
        "  evaluators=\"[]\" \\\n",
        "  epochs=50000 \\\n",
        "  eval_interval=5000 \\\n",
        "  lr=1e-4 \\\n",
        "  puzzle_emb_lr=1e-4 \\\n",
        "  weight_decay=1.0 \\\n",
        "  puzzle_emb_weight_decay=1.0 \\\n",
        "  arch.mlp_t=True \\\n",
        "  arch.pos_encodings=none \\\n",
        "  arch.L_layers=2 \\\n",
        "  arch.H_cycles=3 \\\n",
        "  arch.L_cycles=6 \\\n",
        "  +run_name={run_name} \\\n",
        "  ema=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx9ZGUkDv7Fb"
      },
      "outputs": [],
      "source": [
        "# Train on Sudoku-Extreme (Attention version)\n",
        "# Runtime: <36 hours on 1 L40S GPU\n",
        "\n",
        "run_name = \"pretrain_att_sudoku\"\n",
        "\n",
        "!python pretrain.py \\\n",
        "  arch=trm \\\n",
        "  data_paths=\"[data/sudoku-extreme-1k-aug-1000]\" \\\n",
        "  evaluators=\"[]\" \\\n",
        "  epochs=50000 \\\n",
        "  eval_interval=5000 \\\n",
        "  lr=1e-4 \\\n",
        "  puzzle_emb_lr=1e-4 \\\n",
        "  weight_decay=1.0 \\\n",
        "  puzzle_emb_weight_decay=1.0 \\\n",
        "  arch.L_layers=2 \\\n",
        "  arch.H_cycles=3 \\\n",
        "  arch.L_cycles=6 \\\n",
        "  +run_name={run_name} \\\n",
        "  ema=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uaudtj-vv7Fb"
      },
      "source": [
        "### Maze-Hard Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRE8brJ9v7Fb"
      },
      "outputs": [],
      "source": [
        "# Train on Maze-Hard\n",
        "# Runtime: <24 hours on 4 L40S GPUs\n",
        "\n",
        "run_name = \"pretrain_att_maze30x30\"\n",
        "\n",
        "!torchrun --nproc-per-node 4 \\\n",
        "  --rdzv_backend=c10d \\\n",
        "  --rdzv_endpoint=localhost:0 \\\n",
        "  --nnodes=1 \\\n",
        "  pretrain.py \\\n",
        "  arch=trm \\\n",
        "  data_paths=\"[data/maze-30x30-hard-1k]\" \\\n",
        "  evaluators=\"[]\" \\\n",
        "  epochs=50000 \\\n",
        "  eval_interval=5000 \\\n",
        "  lr=1e-4 \\\n",
        "  puzzle_emb_lr=1e-4 \\\n",
        "  weight_decay=1.0 \\\n",
        "  puzzle_emb_weight_decay=1.0 \\\n",
        "  arch.L_layers=2 \\\n",
        "  arch.H_cycles=3 \\\n",
        "  arch.L_cycles=4 \\\n",
        "  +run_name={run_name} \\\n",
        "  ema=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTVawg_Cv7Fc"
      },
      "source": [
        "## Evaluation and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dS-38Wov7Fd"
      },
      "outputs": [],
      "source": [
        "# Monitor Training Progress\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# List all run directories\n",
        "runs = glob.glob(\"outputs/*/\")\n",
        "print(\"Available training runs:\")\n",
        "for run in sorted(runs):\n",
        "    print(f\"  {run}\")\n",
        "\n",
        "# Check latest checkpoint\n",
        "latest_run = max(runs, key=os.path.getmtime) if runs else None\n",
        "if latest_run:\n",
        "    print(f\"\\nLatest run: {latest_run}\")\n",
        "    checkpoints = glob.glob(os.path.join(latest_run, \"*.pt\"))\n",
        "    print(f\"Checkpoints: {len(checkpoints)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMbPgF1Nv7Fd"
      },
      "outputs": [],
      "source": [
        "# Load and Evaluate Model\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Specify checkpoint path\n",
        "checkpoint_path = \"outputs/YOUR_RUN_NAME/checkpoint_best.pt\"\n",
        "\n",
        "if Path(checkpoint_path).exists():\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    print(f\"Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
        "    print(f\"Best validation accuracy: {checkpoint.get('best_val_acc', 'unknown')}\")\n",
        "else:\n",
        "    print(f\"Checkpoint not found: {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_iZvHHTv7Fe"
      },
      "outputs": [],
      "source": [
        "# Visualize Results\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_arc_prediction(input_grid, true_output, predicted_output):\n",
        "    \"\"\"Visualize ARC-AGI predictions\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    axes[0].imshow(input_grid, cmap='tab10', interpolation='nearest')\n",
        "    axes[0].set_title('Input')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(true_output, cmap='tab10', interpolation='nearest')\n",
        "    axes[1].set_title('True Output')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(predicted_output, cmap='tab10', interpolation='nearest')\n",
        "    axes[2].set_title('Predicted Output')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage (with dummy data)\n",
        "# input_grid = np.random.randint(0, 10, (10, 10))\n",
        "# true_output = np.random.randint(0, 10, (10, 10))\n",
        "# predicted_output = np.random.randint(0, 10, (10, 10))\n",
        "# visualize_arc_prediction(input_grid, true_output, predicted_output)\n",
        "\n",
        "print(\"Visualization functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6XhCx4Pv7Ff"
      },
      "source": [
        "## Citation\n",
        "\n",
        "If you use this code, please cite:\n",
        "\n",
        "```bibtex\n",
        "@misc{jolicoeurmartineau2025morerecursivereasoningtiny,\n",
        "      title={Less is More: Recursive Reasoning with Tiny Networks},\n",
        "      author={Alexia Jolicoeur-Martineau},\n",
        "      year={2025},\n",
        "      eprint={2510.04871},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.LG},\n",
        "      url={https://arxiv.org/abs/2510.04871},\n",
        "}\n",
        "```\n",
        "\n",
        "And the Hierarchical Reasoning Model (HRM):\n",
        "\n",
        "```bibtex\n",
        "@misc{wang2025hierarchicalreasoningmodel,\n",
        "      title={Hierarchical Reasoning Model},\n",
        "      author={Guan Wang and Jin Li and Yuhao Sun and Xing Chen and Changling Liu and Yue Wu and Meng Lu and Sen Song and Yasin Abbasi Yadkori},\n",
        "      year={2025},\n",
        "      eprint={2506.21734},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.AI},\n",
        "      url={https://arxiv.org/abs/2506.21734},\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}