{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weagan/Tiny-Recursive-Models/blob/main/TRM_Demonstration_(4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dBLIOn5KGzo"
      },
      "source": [
        "# Understanding Tiny Recursive Models (TRM)\n",
        "\n",
        "This Colab notebook provides a hands-on implementation of a Tiny Recursive Model (TRM), based on the concepts outlined in the [learn-tiny-recursive-models GitHub repository](https://github.com/vukrosic/learn-tiny-recursive-models).\n",
        "\n",
        "We will:\n",
        "1.  Implement the core `RecursiveBlock`.\n",
        "2.  Build the full `TRM` model.\n",
        "3.  Run a forward pass to see how it processes a sequence.\n",
        "4.  Set up a simple training loop to watch the model learn.\n",
        "5.  Train the model on a more complex task: solving 5x5 mazes.\n"
      ],
      "id": "-dBLIOn5KGzo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPgSaLW-KGzq"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's ensure we have PyTorch installed. Google Colab usually comes with it pre-installed, but it's good practice to run the installation command.\n"
      ],
      "id": "kPgSaLW-KGzq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9y3UuycKGzr",
        "outputId": "71577141-1233-40ec-e815-621a3e6ff48d"
      },
      "source": [
        "!pip install -q torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "-9y3UuycKGzr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hBrnWa7KGzs"
      },
      "source": [
        "## 1. The Core Component: `RecursiveBlock`\n",
        "\n",
        "The fundamental building block of a TRM is the `RecursiveBlock`. It takes an input tensor and a hidden state from the previous step, and produces an output and an updated hidden state. This recursive nature allows it to process sequences step-by-step.\n",
        "\n",
        "The block consists of:\n",
        "-   Two Layer Normalization layers for stability.\n",
        "-   Two Linear layers to transform the concatenated input and state.\n",
        "-   A GELU activation function for non-linearity.\n"
      ],
      "id": "-hBrnWa7KGzs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbo9icf1KGzs"
      },
      "source": [
        "class RecursiveBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single recursive block for the TRM.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Layer normalization for the state and input\n",
        "        self.norm_state = nn.LayerNorm(d_model)\n",
        "        self.norm_input = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Linear layers to process the combined state and input\n",
        "        # The input dimension is 2 * d_model because we concatenate state and input\n",
        "        self.linear1 = nn.Linear(2 * d_model, 2 * d_model)\n",
        "        self.activation = nn.GELU()\n",
        "        self.linear2 = nn.Linear(2 * d_model, 2 * d_model)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        \"\"\"\n",
        "        Forward pass for the recursive block.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor for the current step. Shape: (batch_size, d_model)\n",
        "            state (torch.Tensor): Hidden state from the previous step. Shape: (batch_size, d_model)\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: The output and the new state. Both have shape (batch_size, d_model)\n",
        "        \"\"\"\n",
        "        # Normalize the state and input separately\n",
        "        normalized_state = self.norm_state(state)\n",
        "        normalized_input = self.norm_input(x)\n",
        "\n",
        "        # Concatenate along the feature dimension\n",
        "        combined_input = torch.cat([normalized_state, normalized_input], dim=1)\n",
        "\n",
        "        # Pass through the linear layers\n",
        "        hidden = self.linear1(combined_input)\n",
        "        hidden = self.activation(hidden)\n",
        "        processed_output = self.linear2(hidden)\n",
        "\n",
        "        # The magic of TRM: the new state and output are derived from the same processed tensor.\n",
        "        # This is a simple but effective way to update the state.\n",
        "        new_state = state + processed_output[:, :self.d_model]\n",
        "        output = processed_output[:, self.d_model:]\n",
        "\n",
        "        return output, new_state\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "qbo9icf1KGzs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NDN-OhFKGzs"
      },
      "source": [
        "## 2. The TRM Model: Processing Sequences\n",
        "\n",
        "The full TRM model wraps the `RecursiveBlock`. It initializes the hidden state (usually with zeros) and then iterates through the input sequence, feeding each element into the recursive block one at a time. This step-by-step processing is the core of its operation.\n",
        "\n",
        "We also add input and output embedding layers to map our vocabulary to the model's dimension and back.\n"
      ],
      "id": "2NDN-OhFKGzs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA0gGQmdKGzt"
      },
      "source": [
        "class TRM(nn.Module):\n",
        "    \"\"\"\n",
        "    The Tiny Recursive Model.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Embedding layer to convert token IDs to vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # The recursive block\n",
        "        self.recursive_block = RecursiveBlock(d_model)\n",
        "\n",
        "        # A final layer normalization and linear layer to produce logits\n",
        "        self.norm_out = nn.LayerNorm(d_model)\n",
        "        self.output_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        \"\"\"\n",
        "        Forward pass for the entire sequence.\n",
        "        Args:\n",
        "            input_sequence (torch.Tensor): A sequence of token IDs. Shape: (batch_size, seq_len)\n",
        "        Returns:\n",
        "            torch.Tensor: The output logits for each step in the sequence. Shape: (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_sequence.shape\n",
        "\n",
        "        # 1. Embed the input sequence\n",
        "        embedded_input = self.embedding(input_sequence)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 2. Initialize the hidden state with zeros\n",
        "        state = torch.zeros(batch_size, self.d_model, device=input_sequence.device)\n",
        "\n",
        "        # 3. Process the sequence step-by-step\n",
        "        outputs = []\n",
        "        for i in range(seq_len):\n",
        "            # Get the input for the current time step\n",
        "            step_input = embedded_input[:, i, :]  # Shape: (batch_size, d_model)\n",
        "\n",
        "            # Pass through the recursive block\n",
        "            output, state = self.recursive_block(step_input, state)\n",
        "            outputs.append(output)\n",
        "\n",
        "        # 4. Stack the outputs and project to vocabulary size\n",
        "        # Stack along the sequence dimension\n",
        "        outputs_tensor = torch.stack(outputs, dim=1)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Final normalization and linear projection\n",
        "        normalized_outputs = self.norm_out(outputs_tensor)\n",
        "        logits = self.output_linear(normalized_outputs)\n",
        "\n",
        "        return logits\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "qA0gGQmdKGzt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnK0TamAKGzt"
      },
      "source": [
        "## 3. Forward Pass Demonstration\n",
        "\n",
        "Let's create a dummy input sequence and pass it through our model to see the shapes of the tensors at each step. This helps verify that our implementation is working correctly.\n"
      ],
      "id": "nnK0TamAKGzt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCEyVrSEKGzt",
        "outputId": "afb94ddb-0d25-4ea4-e510-4edc2bbe4c68"
      },
      "source": [
        "# Model parameters\n",
        "VOCAB_SIZE = 20\n",
        "D_MODEL = 32\n",
        "SEQ_LEN = 5\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# Instantiate the model\n",
        "model = TRM(vocab_size=VOCAB_SIZE, d_model=D_MODEL)\n",
        "print(\"Model Architecture:\")\n",
        "print(model)\n",
        "\n",
        "# Create a dummy input sequence (batch_size, seq_len)\n",
        "# These are random token IDs from our vocabulary\n",
        "dummy_input = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LEN))\n",
        "print(f\"\\nDummy Input Shape: {dummy_input.shape}\")\n",
        "print(f\"Dummy Input Tensor:\\n{dummy_input}\")\n",
        "\n",
        "# Perform a forward pass\n",
        "output_logits = model(dummy_input)\n",
        "\n",
        "print(f\"\\nOutput Logits Shape: {output_logits.shape}\")\n",
        "print(\"This shape (batch_size, seq_len, vocab_size) is what we expect!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Architecture:\n",
            "TRM(\n",
            "  (embedding): Embedding(20, 32)\n",
            "  (recursive_block): RecursiveBlock(\n",
            "    (norm_state): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm_input): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (activation): GELU(approximate='none')\n",
            "    (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  )\n",
            "  (norm_out): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "  (output_linear): Linear(in_features=32, out_features=20, bias=True)\n",
            ")\n",
            "\n",
            "Dummy Input Shape: torch.Size([1, 5])\n",
            "Dummy Input Tensor:\n",
            "tensor([[10,  9, 12, 12,  4]])\n",
            "\n",
            "Output Logits Shape: torch.Size([1, 5, 20])\n",
            "This shape (batch_size, seq_len, vocab_size) is what we expect!\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "iCEyVrSEKGzt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBJTQSVcKGzt"
      },
      "source": [
        "## 4. A Simple Training Example\n",
        "\n",
        "To show that the model can learn, let's create a simple \"next token prediction\" task. Our goal is to train the model to predict the next number in a sequence.\n",
        "\n",
        "-   **Input:** `[1, 2, 3, 4]`\n",
        "-   **Target:** `[2, 3, 4, 5]`\n",
        "\n",
        "We'll use Cross-Entropy Loss and the Adam optimizer.\n"
      ],
      "id": "tBJTQSVcKGzt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN0VfovnKGzt",
        "outputId": "cdbf9999-b099-4370-f4df-a0f5a18eb32d"
      },
      "source": [
        "# --- Training Setup ---\n",
        "# Let's use a slightly bigger vocabulary for this task\n",
        "TRAINING_VOCAB_SIZE = 10\n",
        "TRAINING_D_MODEL = 16\n",
        "\n",
        "# Create a new model instance for training\n",
        "training_model = TRM(vocab_size=TRAINING_VOCAB_SIZE, d_model=TRAINING_D_MODEL)\n",
        "optimizer = optim.Adam(training_model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Data ---\n",
        "# Our simple sequence prediction task\n",
        "input_data = torch.tensor([[1, 2, 3, 4]])      # Shape: (1, 4)\n",
        "target_data = torch.tensor([[2, 3, 4, 5]])     # Shape: (1, 4)\n",
        "\n",
        "print(f\"Input:  {input_data[0].tolist()}\")\n",
        "print(f\"Target: {target_data[0].tolist()}\")\n",
        "\n",
        "# --- Training Loop ---\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    logits = training_model(input_data) # Shape: (batch, seq_len, vocab_size)\n",
        "\n",
        "    # Reshape for the loss function\n",
        "    # The loss function expects (N, C) where C is number of classes\n",
        "    # Logits: (1, 4, 10) -> (4, 10)\n",
        "    # Target: (1, 4) -> (4)\n",
        "    loss = criterion(logits.view(-1, TRAINING_VOCAB_SIZE), target_data.view(-1))\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# --- Inference after training ---\n",
        "print(\"\\n--- Testing after training ---\")\n",
        "with torch.no_grad():\n",
        "    test_input = torch.tensor([[1, 2, 3, 4]])\n",
        "    predictions = training_model(test_input)\n",
        "    # Get the predicted token ID by finding the max logit\n",
        "    predicted_ids = torch.argmax(predictions, dim=2)\n",
        "\n",
        "    print(f\"Input:           {test_input[0].tolist()}\")\n",
        "    print(f\"Predicted sequence: {predicted_ids[0].tolist()}\")\n",
        "    print(f\"Target sequence:    {target_data[0].tolist()}\")\n",
        "    print(\"\\nThe model has learned to predict the next number in the sequence!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  [1, 2, 3, 4]\n",
            "Target: [2, 3, 4, 5]\n",
            "Epoch [10/100], Loss: 0.2567\n",
            "Epoch [20/100], Loss: 0.0523\n",
            "Epoch [30/100], Loss: 0.0164\n",
            "Epoch [40/100], Loss: 0.0082\n",
            "Epoch [50/100], Loss: 0.0055\n",
            "Epoch [60/100], Loss: 0.0042\n",
            "Epoch [70/100], Loss: 0.0035\n",
            "Epoch [80/100], Loss: 0.0030\n",
            "Epoch [90/100], Loss: 0.0027\n",
            "Epoch [100/100], Loss: 0.0024\n",
            "\n",
            "--- Testing after training ---\n",
            "Input:           [1, 2, 3, 4]\n",
            "Predicted sequence: [2, 3, 4, 5]\n",
            "Target sequence:    [2, 3, 4, 5]\n",
            "\n",
            "The model has learned to predict the next number in the sequence!\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "UN0VfovnKGzt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHJMXCBbKGzu"
      },
      "source": [
        "## 5. Advanced Example: Solving 5x5 Mazes\n",
        "\n",
        "Now for a more challenging task. Let's train the TRM to solve simple 5x5 mazes.\n",
        "\n",
        "**The Task:** The model will be given a sequence representing the maze structure, followed by the starting position of the path. Its goal is to predict the rest of the coordinate sequence that solves the maze.\n",
        "\n",
        "**Data Representation:**\n",
        "We will convert the maze and its solution path into a single sequence of integers (tokens).\n",
        "\n",
        "- **Vocabulary:**\n",
        "- `0`: Wall (`#`)\n",
        "- `1`: Path (`.`)\n",
        "- `2`: Start (`S`)\n",
        "- `3`: End (`E`)\n",
        "- `4`: Separator (`|`) - A special token to divide the maze layout from the path coordinates.\n",
        "- `5-29`: Path Coordinates - Each of the 25 cells `(row, col)` is mapped to a unique token `5 + row * 5 + col`.\n",
        "\n",
        "- **Sequence Format:**\n",
        "- The model is trained on a single continuous sequence. The input is the sequence up to step `n-1`, and the target is the sequence up to step `n` (shifted).\n",
        "- We only care about predicting the path. Therefore, we will use a **loss mask** to ignore the model's predictions for the maze layout part of the sequence during training.\n"
      ],
      "id": "uHJMXCBbKGzu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAKVo9IYKGzu",
        "outputId": "52ec6040-8b04-4a34-e7bd-3dd3661f130c"
      },
      "source": [
        "# --- Maze Data Setup ---\n",
        "\n",
        "# Define a small dataset of 5x5 mazes and their solutions\n",
        "# 0: Wall, 1: Path, 2: Start, 3: End\n",
        "MAZE_DATASET = [\n",
        "    {\n",
        "        \"maze\": [\n",
        "            [2, 1, 0, 0, 0],\n",
        "            [1, 1, 1, 1, 0],\n",
        "            [0, 0, 0, 1, 0],\n",
        "            [0, 1, 1, 1, 1],\n",
        "            [0, 0, 0, 0, 3],\n",
        "        ],\n",
        "        \"path\": [(0,0), (1,0), (1,1), (1,2), (1,3), (2,3), (3,3), (3,4), (4,4)]\n",
        "    },\n",
        "    {\n",
        "        \"maze\": [\n",
        "            [2, 0, 0, 0, 0],\n",
        "            [1, 1, 1, 1, 1],\n",
        "            [0, 0, 0, 0, 1],\n",
        "            [1, 1, 1, 1, 1],\n",
        "            [1, 0, 0, 0, 3],\n",
        "        ],\n",
        "        \"path\": [(0,0), (1,0), (1,1), (1,2), (1,3), (1,4), (2,4), (3,4), (3,3), (3,2), (3,1), (3,0), (4,0), (4,4)]\n",
        "    },\n",
        "    {\n",
        "        \"maze\": [\n",
        "            [2, 1, 1, 1, 1],\n",
        "            [0, 1, 0, 0, 1],\n",
        "            [1, 1, 1, 0, 1],\n",
        "            [1, 0, 1, 1, 1],\n",
        "            [1, 1, 1, 0, 3],\n",
        "        ],\n",
        "        \"path\": [(0,0), (0,1), (0,2), (0,3), (0,4), (1,4), (2,4), (2,2), (2,1), (2,0), (3,0), (4,0), (4,1), (4,2), (4,4)]\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Vocabulary and Preprocessing ---\n",
        "WALL, PATH, START, END, SEP = 0, 1, 2, 3, 4\n",
        "PATH_TOKEN_OFFSET = 5\n",
        "MAZE_SIZE = 5\n",
        "MAZE_TOKENS = MAZE_SIZE * MAZE_SIZE\n",
        "\n",
        "def preprocess_maze_data(dataset):\n",
        "    sequences = []\n",
        "    for item in dataset:\n",
        "        maze_flat = [token for row in item[\"maze\"] for token in row]\n",
        "        path_tokens = [PATH_TOKEN_OFFSET + r * MAZE_SIZE + c for r, c in item[\"path\"]]\n",
        "\n",
        "        full_sequence = maze_flat + [SEP] + path_tokens\n",
        "        sequences.append(torch.tensor(full_sequence))\n",
        "    return sequences\n",
        "\n",
        "training_sequences = preprocess_maze_data(MAZE_DATASET)\n",
        "\n",
        "# Let's inspect the first preprocessed sequence\n",
        "print(\"Original Maze (first example):\")\n",
        "for row in MAZE_DATASET[0]['maze']:\n",
        "    print(\"\".join([{0:'#', 1:'.', 2:'S', 3:'E'}[c] for c in row]))\n",
        "\n",
        "print(\"\\nPreprocessed sequence (tokens):\")\n",
        "print(training_sequences[0])\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Maze (first example):\n",
            "S.###\n",
            "....#\n",
            "###.#\n",
            "#....\n",
            "####E\n",
            "\n",
            "Preprocessed sequence (tokens):\n",
            "tensor([ 2,  1,  0,  0,  0,  1,  1,  1,  1,  0,  0,  0,  0,  1,  0,  0,  1,  1,\n",
            "         1,  1,  0,  0,  0,  0,  3,  4,  5, 10, 11, 12, 13, 18, 23, 24, 29])\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "rAKVo9IYKGzu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0utjDygFKGzu",
        "outputId": "2a53c82f-ca40-446d-a9ca-986bf8d4d9f3"
      },
      "source": [
        "# --- Maze Model Training ---\n",
        "MAZE_VOCAB_SIZE = PATH_TOKEN_OFFSET + MAZE_TOKENS # 5 special tokens + 25 path tokens\n",
        "MAZE_D_MODEL = 32\n",
        "\n",
        "maze_model = TRM(vocab_size=MAZE_VOCAB_SIZE, d_model=MAZE_D_MODEL)\n",
        "maze_optimizer = optim.Adam(maze_model.parameters(), lr=0.005)\n",
        "maze_criterion = nn.CrossEntropyLoss(reduction='none') # Use 'none' to apply mask later\n",
        "\n",
        "epochs = 500\n",
        "print(\"Starting maze training...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for seq in training_sequences:\n",
        "        maze_optimizer.zero_grad()\n",
        "\n",
        "        # Prepare input and target (shifted input)\n",
        "        input_seq = seq[:-1].unsqueeze(0)\n",
        "        target_seq = seq[1:].unsqueeze(0)\n",
        "\n",
        "        # Define the loss mask: we only care about predicting the path tokens.\n",
        "        # The path starts after the maze layout (25 tokens) and the SEP token.\n",
        "        # The target for the SEP token is the first path token, so we start the mask there.\n",
        "        loss_mask = torch.zeros_like(target_seq, dtype=torch.float)\n",
        "        loss_mask[:, MAZE_TOKENS:] = 1.0\n",
        "\n",
        "        # Forward pass\n",
        "        logits = maze_model(input_seq)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = maze_criterion(logits.view(-1, MAZE_VOCAB_SIZE), target_seq.view(-1))\n",
        "        masked_loss = loss * loss_mask.view(-1)\n",
        "\n",
        "        # Average the loss over the number of path tokens only\n",
        "        final_loss = masked_loss.sum() / loss_mask.sum()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        final_loss.backward()\n",
        "        maze_optimizer.step()\n",
        "\n",
        "        total_loss += masked_loss.sum().item()\n",
        "        total_tokens += loss_mask.sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Maze training finished.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting maze training...\n",
            "Epoch [50/500], Average Loss: 0.0160\n",
            "Epoch [100/500], Average Loss: 0.0038\n",
            "Epoch [150/500], Average Loss: 0.0019\n",
            "Epoch [200/500], Average Loss: 0.0011\n",
            "Epoch [250/500], Average Loss: 0.0008\n",
            "Epoch [300/500], Average Loss: 0.0006\n",
            "Epoch [350/500], Average Loss: 0.0004\n",
            "Epoch [400/500], Average Loss: 0.0003\n",
            "Epoch [450/500], Average Loss: 0.0003\n",
            "Epoch [500/500], Average Loss: 0.0002\n",
            "Maze training finished.\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "0utjDygFKGzu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wugHeukjKGzu",
        "outputId": "c84c7605-0c7b-4def-8eb3-6f41ee0aa9d5"
      },
      "source": [
        "# --- Inference on a Maze ---\n",
        "def solve_maze(model, maze_grid, max_len=25):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    maze_flat = [token for row in maze_grid for token in row]\n",
        "\n",
        "    # Find start position to begin generation\n",
        "    start_pos_flat = maze_flat.index(START)\n",
        "    start_r, start_c = divmod(start_pos_flat, MAZE_SIZE)\n",
        "    start_path_token = PATH_TOKEN_OFFSET + start_r * MAZE_SIZE + start_c\n",
        "\n",
        "    # Initial input: maze layout + separator + start token\n",
        "    input_seq = torch.tensor(maze_flat + [SEP] + [start_path_token]).unsqueeze(0)\n",
        "\n",
        "    generated_path_tokens = [start_path_token]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - 1):\n",
        "            logits = model(input_seq)\n",
        "\n",
        "            # Get the prediction for the very last token in the sequence\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            predicted_token = torch.argmax(next_token_logits, dim=1).item()\n",
        "\n",
        "            # Append the prediction and update the input for the next step\n",
        "            generated_path_tokens.append(predicted_token)\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[predicted_token]])], dim=1)\n",
        "\n",
        "            # Stop if we predict the end token\n",
        "            end_pos_flat = maze_flat.index(END)\n",
        "            end_r, end_c = divmod(end_pos_flat, MAZE_SIZE)\n",
        "            end_path_token = PATH_TOKEN_OFFSET + end_r * MAZE_SIZE + end_c\n",
        "            if predicted_token == end_path_token:\n",
        "                break\n",
        "\n",
        "    # Convert token IDs back to (row, col) coordinates\n",
        "    path_coords = []\n",
        "    for token in generated_path_tokens:\n",
        "        flat_pos = token - PATH_TOKEN_OFFSET\n",
        "        r, c = divmod(flat_pos, MAZE_SIZE)\n",
        "        path_coords.append((r, c))\n",
        "\n",
        "    return path_coords\n",
        "\n",
        "# Test with the first maze from our dataset\n",
        "test_maze_grid = MAZE_DATASET[0][\"maze\"]\n",
        "predicted_path = solve_maze(maze_model, test_maze_grid)\n",
        "\n",
        "print(\"Maze to solve:\")\n",
        "for row in test_maze_grid:\n",
        "     print(\"\".join([{0:'#', 1:'.', 2:'S', 3:'E'}[c] for c in row]))\n",
        "\n",
        "print(f\"\\nPredicted Path (coordinates):\\n{predicted_path}\")\n",
        "print(f\"\\nCorrect Path (coordinates):\\n{MAZE_DATASET[0]['path']}\")\n",
        "\n",
        "# Visualize the path\n",
        "print(\"\\nVisualized solution (* = predicted path):\")\n",
        "solution_grid = [[' ' for _ in range(MAZE_SIZE)] for _ in range(MAZE_SIZE)]\n",
        "for r in range(MAZE_SIZE):\n",
        "    line = \"\"\n",
        "    for c in range(MAZE_SIZE):\n",
        "        if test_maze_grid[r][c] == WALL:\n",
        "            line += '#'\n",
        "        elif (r,c) in predicted_path:\n",
        "            line += '*'\n",
        "        else:\n",
        "            line += '.'\n",
        "    print(line)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maze to solve:\n",
            "S.###\n",
            "....#\n",
            "###.#\n",
            "#....\n",
            "####E\n",
            "\n",
            "Predicted Path (coordinates):\n",
            "[(0, 0), (1, 0), (1, 1), (1, 2), (1, 3), (2, 3), (3, 3), (3, 4), (4, 4)]\n",
            "\n",
            "Correct Path (coordinates):\n",
            "[(0, 0), (1, 0), (1, 1), (1, 2), (1, 3), (2, 3), (3, 3), (3, 4), (4, 4)]\n",
            "\n",
            "Visualized solution (* = predicted path):\n",
            "*.###\n",
            "****#\n",
            "###*#\n",
            "#..**\n",
            "####*\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "wugHeukjKGzu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yXiK2rcKGzu"
      },
      "source": [
        "The model learns to generate the sequence of coordinates, effectively solving the maze. This demonstrates the TRM's ability to handle more structured sequence-to-sequence tasks that require understanding a context (the maze layout) to generate a relevant output (the path).\n"
      ],
      "id": "-yXiK2rcKGzu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCtoyvYiKGzv"
      },
      "source": [
        "## 6. Key Differences from Transformers\n",
        "\n",
        "As summarized in the original repository, TRMs differ from Transformers in several key ways:\n",
        "\n",
        "-   **Computation:** TRMs are **recursive** and process sequences step-by-step, making their complexity linear, O(L). Transformers use **self-attention**, which is parallelizable but has quadratic complexity, O(LÂ²).\n",
        "-   **State Management:** TRMs explicitly manage a **hidden state** that evolves over time. Transformers are stateless and recompute context from scratch at each layer.\n",
        "-   **Positional Information:** TRMs inherently understand sequence order due to their sequential processing. Transformers require explicit **positional encodings** to be added to their inputs.\n"
      ],
      "id": "qCtoyvYiKGzv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuCcBKn9KGzv"
      },
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "This notebook provided a brief, practical introduction to Tiny Recursive Models. We implemented the core components in PyTorch, demonstrated that a simple TRM can learn a basic sequence prediction task, and showed its effectiveness on a more complex maze-solving problem.\n",
        "\n",
        "TRMs offer an interesting alternative to Transformers, particularly for applications where computational efficiency and explicit state management are important.\n"
      ],
      "id": "kuCcBKn9KGzv"
    }
  ]
}