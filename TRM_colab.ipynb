{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weagan/Tiny-Recursive-Models/blob/main/TRM_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRM Sudoku Solver - Direct Colab Version\n",
        "# Run this in a single Colab cell to test the basic functionality\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q torch torchvision tqdm pandas kaggle\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFvHuvXyaqVY",
        "outputId": "88bec25e-6fd1-40a5-b582-4f6e75cb658f"
      },
      "id": "mFvHuvXyaqVY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.8.0+cu126\n",
            "CUDA: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download dataset directly\n",
        "!wget -q https://github.com/bryanpark/sudoku/raw/master/sudoku.csv\n",
        "!ls -lh sudoku.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Gp0KQeoatUd",
        "outputId": "86a941fe-49f0-4476-e38a-9cc25337ac5d"
      },
      "id": "6Gp0KQeoatUd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'sudoku.csv': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model implementation\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = self.dropout(F.softmax(scores, dim=-1))\n",
        "        out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1, use_attention=True):\n",
        "        super().__init__()\n",
        "        self.use_attention = use_attention\n",
        "        if use_attention:\n",
        "            self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "            self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        if self.use_attention:\n",
        "            x = self.norm1(x + self.dropout(self.attn(x, mask)))\n",
        "        x = self.norm2(x + self.dropout(self.ffn(x)))\n",
        "        return x\n",
        "\n",
        "class TRM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, n_heads=4, d_ff=512, n_layers=4,\n",
        "                 n_reasoning_steps=4, n_refinement_steps=8, latent_len=16,\n",
        "                 use_attention=True, tie_embeddings=True):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.latent_len = latent_len\n",
        "        self.n_reasoning_steps = n_reasoning_steps\n",
        "        self.n_refinement_steps = n_refinement_steps\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(512, d_model)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, d_ff, use_attention=use_attention) for _ in range(n_layers)])\n",
        "        self.reverse_embedding = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        if tie_embeddings:\n",
        "            self.reverse_embedding.weight = self.token_embedding.weight\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Embedding):\n",
        "                nn.init.normal_(m.weight, std=0.02)\n",
        "\n",
        "    def apply_blocks(self, x, mask=None):\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "        return x\n",
        "\n",
        "    def forward_pass(self, x, y, z, mask=None):\n",
        "        combined = torch.cat([x, y, z], dim=1)\n",
        "        combined = self.apply_blocks(combined, mask)\n",
        "        len_x, len_y = x.size(1), y.size(1)\n",
        "        return combined[:, :len_x, :], combined[:, len_x:len_x+len_y, :], combined[:, len_x+len_y:, :]\n",
        "\n",
        "    def recursive_reasoning(self, x, y, z, mask=None):\n",
        "        for _ in range(self.n_reasoning_steps):\n",
        "            _, _, z = self.forward_pass(x, y, z, mask)\n",
        "        for _ in range(self.n_refinement_steps):\n",
        "            _, y, _ = self.forward_pass(x, y, z, mask)\n",
        "        return y\n",
        "\n",
        "    def forward(self, question_ids, answer_ids=None, mask=None):\n",
        "        batch, qlen = question_ids.size()\n",
        "        device = question_ids.device\n",
        "        x = self.token_embedding(question_ids) + self.position_embedding(torch.arange(qlen, device=device).unsqueeze(0))\n",
        "        if answer_ids is not None:\n",
        "            alen = answer_ids.size(1)\n",
        "            y = self.token_embedding(answer_ids) + self.position_embedding(torch.arange(alen, device=device).unsqueeze(0))\n",
        "        else:\n",
        "            y = torch.randn(batch, 81, self.d_model, device=device) * 0.02\n",
        "        z = torch.randn(batch, self.latent_len, self.d_model, device=device) * 0.02\n",
        "        y_final = self.recursive_reasoning(x, y, z, mask)\n",
        "        return self.reverse_embedding(y_final)\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
      ],
      "metadata": {
        "id": "VkZR-hxea1Nt"
      },
      "id": "VkZR-hxea1Nt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Quick test\n",
        "vocab_size = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TRM(vocab_size=vocab_size).to(device)\n",
        "print(f\"Model created with {model.count_parameters()/1e6:.2f}M parameters\")\n",
        "\n",
        "# Test with a small sample\n",
        "print(\"\\nTesting with sample puzzle...\")\n",
        "puzzle = \"004300209005009001070060043006002087190007400050083000600000105003508690042910300\"\n",
        "puzzle_tensor = torch.tensor([int(c) for c in puzzle], dtype=torch.long).unsqueeze(0).to(device)\n",
        "with torch.no_grad():\n",
        "    output = model(puzzle_tensor)\n",
        "    pred = torch.argmax(output, dim=-1).squeeze(0)\n",
        "    print(f\"Input:  {puzzle}\")\n",
        "    print(f\"Output: {''.join(str(d.item()) for d in pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OubSCJZa9hc",
        "outputId": "3eb5b909-33dc-4241-cd13-fcde38af5ce6"
      },
      "id": "_OubSCJZa9hc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created with 0.86M parameters\n",
            "\n",
            "Testing with sample puzzle...\n",
            "Input:  004300209005009001070060043006002087190007400050083000600000105003508690042910300\n",
            "Output: 331141218019542486413855918645166134364486963956413462413820350314561110743543530\n"
          ]
        }
      ]
    }
  ]
}