{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weagan/Tiny-Recursive-Models/blob/main/TRM_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43QbBxf1NUpl"
      },
      "source": [
        "# Tiny Recursive Model (TRM) Implementation\n",
        "Based on **\"Let's Build a Tiny Recursive Model from Scratch\"** by Azhar.\n",
        "\n",
        "This notebook builds the TRM (Transformer Reasoning Model) with 3 streams and recursive reasoning."
      ],
      "id": "43QbBxf1NUpl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEkJRpTxNUpo",
        "outputId": "e14e59b6-2047-44da-d16e-c959e74bca2b"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install torch tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "id": "gEkJRpTxNUpo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIXlSViBNUpq"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "GIXlSViBNUpq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARnldzc5NUpq"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        # reshape for heads\n",
        "        q = q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k = k.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return self.out_proj(out)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ARnldzc5NUpq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLX0FOrPNUpr"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "QLX0FOrPNUpr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iokWz7MVNUpr"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1, use_attention=True):\n",
        "        super().__init__()\n",
        "        self.use_attention = use_attention\n",
        "        if use_attention:\n",
        "            self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "            self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        if self.use_attention:\n",
        "            residual = x\n",
        "            x = self.attn(x, mask)\n",
        "            x = self.norm1(residual + self.dropout(x))\n",
        "        residual = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.norm2(residual + self.dropout(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "iokWz7MVNUpr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMc7a-KvNUpr"
      },
      "source": [
        "class TRM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        d_model=256,\n",
        "        n_heads=4,\n",
        "        d_ff=1024,\n",
        "        n_layers=4,\n",
        "        n_reasoning_steps=8,\n",
        "        n_refinement_steps=16,\n",
        "        latent_len=32,\n",
        "        use_attention=True,\n",
        "        tie_embeddings=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.latent_len = latent_len\n",
        "        self.n_reasoning_steps = n_reasoning_steps\n",
        "        self.n_refinement_steps = n_refinement_steps\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(512, d_model)\n",
        "        self.embedding_dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, n_heads, d_ff, use_attention=use_attention)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.reverse_embedding = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        if tie_embeddings:\n",
        "            self.reverse_embedding.weight = self.token_embedding.weight\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Embedding):\n",
        "                nn.init.normal_(m.weight, std=0.02)\n",
        "\n",
        "    def apply_blocks(self, x, mask=None):\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "        return x\n",
        "\n",
        "    def forward_pass(self, x, y, z, mask=None):\n",
        "        # x, y, z: [batch, seq_len, d_model]\n",
        "        combined = torch.cat([x, y, z], dim=1)\n",
        "        combined = self.apply_blocks(combined, mask)\n",
        "\n",
        "        len_x = x.size(1)\n",
        "        len_y = y.size(1)\n",
        "        # split\n",
        "        x_new = combined[:, :len_x, :]\n",
        "        y_new = combined[:, len_x: len_x + len_y, :]\n",
        "        z_new = combined[:, len_x + len_y:, :]\n",
        "        return x_new, y_new, z_new\n",
        "\n",
        "    def recursive_reasoning(self, x, y, z, mask=None):\n",
        "        # Phase 1: reasoning (update z)\n",
        "        for _ in range(self.n_reasoning_steps):\n",
        "            _, _, z = self.forward_pass(x, y, z, mask)\n",
        "        # Phase 2: refinement (update y)\n",
        "        for _ in range(self.n_refinement_steps):\n",
        "            _, y, _ = self.forward_pass(x, y, z, mask)\n",
        "        return y\n",
        "\n",
        "    def forward(self, question_ids, answer_ids=None, mask=None):\n",
        "        batch, qlen = question_ids.size()\n",
        "        device = question_ids.device\n",
        "        x = self.token_embedding(question_ids) + self.position_embedding(\n",
        "            torch.arange(qlen, device=device).unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        # answer stream initialization\n",
        "        if answer_ids is not None:\n",
        "            y = self.token_embedding(answer_ids)\n",
        "        else:\n",
        "            # random init\n",
        "            y = torch.randn(batch, 32, self.d_model, device=device) * 0.02\n",
        "\n",
        "        # reasoning stream init\n",
        "        z = torch.randn(batch, self.latent_len, self.d_model, device=device) * 0.02\n",
        "\n",
        "        y_final = self.recursive_reasoning(x, y, z, mask)\n",
        "        logits = self.reverse_embedding(y_final)\n",
        "        return logits\n",
        "\n",
        "    def generate(self, question_ids, max_length=50, temperature=1.0):\n",
        "        batch = question_ids.size(0)\n",
        "        device = question_ids.device\n",
        "        generated = torch.zeros(batch, 1, dtype=torch.long, device=device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            logits = self.forward(question_ids, generated)\n",
        "            next_logits = logits[:, -1, :] / temperature\n",
        "            probs = F.softmax(next_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "        return generated\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "JMc7a-KvNUpr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNB4_eQJNUps"
      },
      "source": [
        "def create_trm_att(vocab_size):\n",
        "    return TRM(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=256,\n",
        "        n_heads=4,\n",
        "        d_ff=1024,\n",
        "        n_layers=4,\n",
        "        n_reasoning_steps=8,\n",
        "        n_refinement_steps=16,\n",
        "        latent_len=32,\n",
        "        use_attention=True,\n",
        "        tie_embeddings=True\n",
        "    )\n",
        "\n",
        "def create_trm_mlp(vocab_size):\n",
        "    return TRM(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=256,\n",
        "        n_heads=4,\n",
        "        d_ff=1024,\n",
        "        n_layers=4,\n",
        "        n_reasoning_steps=8,\n",
        "        n_refinement_steps=16,\n",
        "        latent_len=32,\n",
        "        use_attention=False,\n",
        "        tie_embeddings=True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "VNB4_eQJNUps"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSYVwT__NUps",
        "outputId": "00cce3e6-b99b-4d47-ae12-4bef072ef9ee"
      },
      "source": [
        "# Example: build TRM model and count parameters\n",
        "vocab_size = 100  # adjust for your tokenizer / dataset\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = create_trm_att(vocab_size).to(device)\n",
        "print(\"# parameters (millions):\", model.count_parameters() / 1e6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# parameters (millions): 3.315712\n"
          ]
        }
      ],
      "id": "WSYVwT__NUps"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg2YrqnHNUps",
        "outputId": "cd295a5e-55d0-4cc6-c55a-a582d457eb8a"
      },
      "source": [
        "# Dummy training loop (replace with real data)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "batch_size = 8\n",
        "seq_len = 10\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for _ in range(10):\n",
        "        # fake data\n",
        "        q = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
        "        a = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
        "        logits = model(q, a)\n",
        "        # logits: [batch, answer_len, vocab_size]\n",
        "        loss = criterion(logits.view(-1, vocab_size), a.view(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch} — Loss: {total_loss / 10:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 — Loss: 4.1506\n",
            "Epoch 1 — Loss: 3.4262\n",
            "Epoch 2 — Loss: 2.8170\n",
            "Epoch 3 — Loss: 2.3642\n",
            "Epoch 4 — Loss: 1.9402\n"
          ]
        }
      ],
      "id": "vg2YrqnHNUps"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYkHbJGfNUpt"
      },
      "source": [
        "# Inference example (again the very dummy question)\n",
        "model.eval()\n",
        "q = torch.randint(0, vocab_size, (1, seq_len), device=device)\n",
        "with torch.no_grad():\n",
        "    out_ids = model.generate(q, max_length=12, temperature=0.7)\n",
        "print(\"Question IDs:\", q)\n",
        "print(\"Generated IDs:\", out_ids)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "CYkHbJGfNUpt"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RP-k1Lz3s0Oi"
      },
      "id": "RP-k1Lz3s0Oi",
      "execution_count": null,
      "outputs": []
    }
  ]
}