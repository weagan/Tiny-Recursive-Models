{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weagan/Tiny-Recursive-Models/blob/main/TRM_Demonstration_20x20_maze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGQHQ24s-qtQ"
      },
      "source": [
        "# Understanding Tiny Recursive Models (TRM)\n",
        "\n",
        "This Colab notebook provides a hands-on implementation of a Tiny Recursive Model (TRM), based on the concepts outlined in the [learn-tiny-recursive-models GitHub repository](https://github.com/vukrosic/learn-tiny-recursive-models).\n",
        "\n",
        "We will:\n",
        "1.  Implement the core `RecursiveBlock`.\n",
        "2.  Build the full `TRM` model.\n",
        "3.  Run a forward pass to see how it processes a sequence.\n",
        "4.  Set up a simple training loop to watch the model learn.\n",
        "5.  Train the model on a more complex task: solving 20x20 mazes.\n"
      ],
      "id": "PGQHQ24s-qtQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMM8f80-qtS"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's ensure we have PyTorch installed. Google Colab usually comes with it pre-installed, but it's good practice to run the installation command.\n"
      ],
      "id": "sSMM8f80-qtS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un49egX9-qtT",
        "outputId": "21f86280-12ca-4182-c790-d3e4012cb706"
      },
      "source": [
        "!pip install -q torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "un49egX9-qtT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a66efb3"
      },
      "source": [
        "## 8. Leveraging GPU for Faster Training (Optional)"
      ],
      "id": "2a66efb3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "293b70b2"
      },
      "source": [
        "To utilize a GPU, we need to perform two main steps:\n",
        "1.  **Check for GPU availability:** Determine if a CUDA-enabled GPU is present.\n",
        "2.  **Move model and data to GPU:** Transfer the model parameters and input/target tensors to the GPU device."
      ],
      "id": "293b70b2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46f80674",
        "outputId": "4ca7efb2-dc51-4205-91af-549ca16987ff"
      },
      "source": [
        "import torch\n",
        "# Check if CUDA (GPU) is available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "id": "46f80674"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5daf56bf"
      },
      "source": [
        "Now, let's adapt the simple training example to use the GPU if available."
      ],
      "id": "5daf56bf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5e666f3",
        "outputId": "21384922-2f09-4f96-c52e-cae7f2ee0d8c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class RecursiveBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single recursive block for the TRM.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Layer normalization for the state and input\n",
        "        self.norm_state = nn.LayerNorm(d_model)\n",
        "        self.norm_input = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Linear layers to process the combined state and input\n",
        "        # The input dimension is 2 * d_model because we concatenate state and input\n",
        "        self.linear1 = nn.Linear(2 * d_model, 2 * d_model)\n",
        "        self.activation = nn.GELU()\n",
        "        self.linear2 = nn.Linear(2 * d_model, 2 * d_model)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        \"\"\"\n",
        "        Forward pass for the recursive block.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor for the current step. Shape: (batch_size, d_model)\n",
        "            state (torch.Tensor): Hidden state from the previous step. Shape: (batch_size, d_model)\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: The output and the new state. Both have shape (batch_size, d_model)\n",
        "        \"\"\"\n",
        "        # Normalize the state and input separately\n",
        "        normalized_state = self.norm_state(state)\n",
        "        normalized_input = self.norm_input(x)\n",
        "\n",
        "        # Concatenate along the feature dimension\n",
        "        combined_input = torch.cat([normalized_state, normalized_input], dim=1)\n",
        "\n",
        "        # Pass through the linear layers\n",
        "        hidden = self.linear1(combined_input)\n",
        "        hidden = self.activation(hidden)\n",
        "        processed_output = self.linear2(hidden)\n",
        "\n",
        "        # The magic of TRM: the new state and output are derived from the same processed tensor.\n",
        "        # This is a simple but effective way to update the state.\n",
        "        new_state = state + processed_output[:, :self.d_model]\n",
        "        output = processed_output[:, self.d_model:]\n",
        "\n",
        "        return output, new_state\n",
        "\n",
        "class TRM(nn.Module):\n",
        "    \"\"\"\n",
        "    The Tiny Recursive Model.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Embedding layer to convert token IDs to vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # The recursive block\n",
        "        self.recursive_block = RecursiveBlock(d_model)\n",
        "\n",
        "        # A final layer normalization and linear layer to produce logits\n",
        "        self.norm_out = nn.LayerNorm(d_model)\n",
        "        self.output_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        \"\"\"\n",
        "        Forward pass for the entire sequence.\n",
        "        Args:\n",
        "            input_sequence (torch.Tensor): A sequence of token IDs. Shape: (batch_size, seq_len)\n",
        "        Returns:\n",
        "            torch.Tensor: The output logits for each step in the sequence. Shape: (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_sequence.shape\n",
        "\n",
        "        # 1. Embed the input sequence\n",
        "        embedded_input = self.embedding(input_sequence)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 2. Initialize the hidden state with zeros\n",
        "        state = torch.zeros(batch_size, self.d_model, device=input_sequence.device)\n",
        "\n",
        "        # 3. Process the sequence step-by-step\n",
        "        outputs = []\n",
        "        for i in range(seq_len):\n",
        "            # Get the input for the current time step\n",
        "            step_input = embedded_input[:, i, :]  # Shape: (batch_size, d_model)\n",
        "\n",
        "            # Pass through the recursive block\n",
        "            output, state = self.recursive_block(step_input, state)\n",
        "            outputs.append(output)\n",
        "\n",
        "        # 4. Stack the outputs and project to vocabulary size\n",
        "        # Stack along the sequence dimension\n",
        "        outputs_tensor = torch.stack(outputs, dim=1)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Final normalization and linear projection\n",
        "        normalized_outputs = self.norm_out(outputs_tensor)\n",
        "        logits = self.output_linear(normalized_outputs)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# --- Training Setup with GPU --- (adapted from previous example)\n",
        "TRAINING_VOCAB_SIZE = 10\n",
        "TRAINING_D_MODEL = 16\n",
        "\n",
        "# Create a new model instance and move it to the device\n",
        "training_model_gpu = TRM(vocab_size=TRAINING_VOCAB_SIZE, d_model=TRAINING_D_MODEL).to(device)\n",
        "optimizer_gpu = optim.Adam(training_model_gpu.parameters(), lr=0.01)\n",
        "criterion_gpu = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Data ---\n",
        "# Move input and target data to the device\n",
        "input_data_gpu = torch.tensor([[1, 2, 3, 4]]).to(device)\n",
        "target_data_gpu = torch.tensor([[2, 3, 4, 5]]).to(device)\n",
        "\n",
        "print(f\"Input on device: {input_data_gpu.device}\")\n",
        "print(f\"Target on device: {target_data_gpu.device}\")\n",
        "print(f\"Model on device: {next(training_model_gpu.parameters()).device}\")\n",
        "\n",
        "# --- Training Loop --- (identical to CPU version, but with GPU tensors)\n",
        "epochs = 100\n",
        "print(\"\\n--- Training on GPU/CPU ---\")\n",
        "for epoch in range(epochs):\n",
        "    optimizer_gpu.zero_grad()\n",
        "    logits = training_model_gpu(input_data_gpu)\n",
        "    loss = criterion_gpu(logits.view(-1, TRAINING_VOCAB_SIZE), target_data_gpu.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer_gpu.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# --- Inference after training with GPU --- (adapted)\n",
        "print(\"\\n--- Testing after training (GPU/CPU) ---\")\n",
        "with torch.no_grad():\n",
        "    test_input_gpu = torch.tensor([[1, 2, 3, 4]]).to(device)\n",
        "    predictions_gpu = training_model_gpu(test_input_gpu)\n",
        "    predicted_ids_gpu = torch.argmax(predictions_gpu, dim=2)\n",
        "\n",
        "    print(f\"Input:              {test_input_gpu[0].tolist()}\")\n",
        "    print(f\"Predicted sequence: {predicted_ids_gpu[0].tolist()}\")\n",
        "    print(f\"Target sequence:    {target_data_gpu[0].tolist()}\")\n",
        "    print(\"The model has learned to predict the next number in the sequence (on GPU/CPU)!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input on device: cuda:0\n",
            "Target on device: cuda:0\n",
            "Model on device: cuda:0\n",
            "\n",
            "--- Training on GPU/CPU ---\n",
            "Epoch [10/100], Loss: 0.3100\n",
            "Epoch [20/100], Loss: 0.0621\n",
            "Epoch [30/100], Loss: 0.0187\n",
            "Epoch [40/100], Loss: 0.0090\n",
            "Epoch [50/100], Loss: 0.0058\n",
            "Epoch [60/100], Loss: 0.0044\n",
            "Epoch [70/100], Loss: 0.0037\n",
            "Epoch [80/100], Loss: 0.0032\n",
            "Epoch [90/100], Loss: 0.0028\n",
            "Epoch [100/100], Loss: 0.0025\n",
            "\n",
            "--- Testing after training (GPU/CPU) ---\n",
            "Input:              [1, 2, 3, 4]\n",
            "Predicted sequence: [2, 3, 4, 5]\n",
            "Target sequence:    [2, 3, 4, 5]\n",
            "The model has learned to predict the next number in the sequence (on GPU/CPU)!\n"
          ]
        }
      ],
      "id": "f5e666f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e827a12e"
      },
      "source": [
        "You would apply the same `.to(device)` method to your `maze_model` and all relevant data tensors (`input_seq`, `target_seq`) in the maze-solving example to train it on a GPU."
      ],
      "id": "e827a12e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD5_G_oC-qtU"
      },
      "source": [
        "## 1. The Core Component: `RecursiveBlock`\n",
        "\n",
        "The fundamental building block of a TRM is the `RecursiveBlock`. It takes an input tensor and a hidden state from the previous step, and produces an output and an updated hidden state. This recursive nature allows it to process sequences step-by-step.\n",
        "\n",
        "The block consists of:\n",
        "-   Two Layer Normalization layers for stability.\n",
        "-   Two Linear layers to transform the concatenated input and state.\n",
        "-   A GELU activation function for non-linearity.\n"
      ],
      "id": "bD5_G_oC-qtU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4t1n1Ke-qtU"
      },
      "source": [
        "class RecursiveBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single recursive block for the TRM.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Layer normalization for the state and input\n",
        "        self.norm_state = nn.LayerNorm(d_model)\n",
        "        self.norm_input = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Linear layers to process the combined state and input\n",
        "        # The input dimension is 2 * d_model because we concatenate state and input\n",
        "        self.linear1 = nn.Linear(2 * d_model, 2 * d_model)\n",
        "        self.activation = nn.GELU()\n",
        "        self.linear2 = nn.Linear(2 * d_model, 2 * d_model)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        \"\"\"\n",
        "        Forward pass for the recursive block.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor for the current step. Shape: (batch_size, d_model)\n",
        "            state (torch.Tensor): Hidden state from the previous step. Shape: (batch_size, d_model)\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: The output and the new state. Both have shape (batch_size, d_model)\n",
        "        \"\"\"\n",
        "        # Normalize the state and input separately\n",
        "        normalized_state = self.norm_state(state)\n",
        "        normalized_input = self.norm_input(x)\n",
        "\n",
        "        # Concatenate along the feature dimension\n",
        "        combined_input = torch.cat([normalized_state, normalized_input], dim=1)\n",
        "\n",
        "        # Pass through the linear layers\n",
        "        hidden = self.linear1(combined_input)\n",
        "        hidden = self.activation(hidden)\n",
        "        processed_output = self.linear2(hidden)\n",
        "\n",
        "        # The magic of TRM: the new state and output are derived from the same processed tensor.\n",
        "        # This is a simple but effective way to update the state.\n",
        "        new_state = state + processed_output[:, :self.d_model]\n",
        "        output = processed_output[:, self.d_model:]\n",
        "\n",
        "        return output, new_state\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "r4t1n1Ke-qtU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIWfxRTq-qtV"
      },
      "source": [
        "## 2. The TRM Model: Processing Sequences\n",
        "\n",
        "The full TRM model wraps the `RecursiveBlock`. It initializes the hidden state (usually with zeros) and then iterates through the input sequence, feeding each element into the recursive block one at a time. This step-by-step processing is the core of its operation.\n",
        "\n",
        "We also add input and output embedding layers to map our vocabulary to the model's dimension and back.\n"
      ],
      "id": "vIWfxRTq-qtV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opmilAGU-qtV"
      },
      "source": [
        "class TRM(nn.Module):\n",
        "    \"\"\"\n",
        "    The Tiny Recursive Model.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Embedding layer to convert token IDs to vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # The recursive block\n",
        "        self.recursive_block = RecursiveBlock(d_model)\n",
        "\n",
        "        # A final layer normalization and linear layer to produce logits\n",
        "        self.norm_out = nn.LayerNorm(d_model)\n",
        "        self.output_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        \"\"\"\n",
        "        Forward pass for the entire sequence.\n",
        "        Args:\n",
        "            input_sequence (torch.Tensor): A sequence of token IDs. Shape: (batch_size, seq_len)\n",
        "        Returns:\n",
        "            torch.Tensor: The output logits for each step in the sequence. Shape: (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_sequence.shape\n",
        "\n",
        "        # 1. Embed the input sequence\n",
        "        embedded_input = self.embedding(input_sequence)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 2. Initialize the hidden state with zeros\n",
        "        state = torch.zeros(batch_size, self.d_model, device=input_sequence.device)\n",
        "\n",
        "        # 3. Process the sequence step-by-step\n",
        "        outputs = []\n",
        "        for i in range(seq_len):\n",
        "            # Get the input for the current time step\n",
        "            step_input = embedded_input[:, i, :]  # Shape: (batch_size, d_model)\n",
        "\n",
        "            # Pass through the recursive block\n",
        "            output, state = self.recursive_block(step_input, state)\n",
        "            outputs.append(output)\n",
        "\n",
        "        # 4. Stack the outputs and project to vocabulary size\n",
        "        # Stack along the sequence dimension\n",
        "        outputs_tensor = torch.stack(outputs, dim=1)  # Shape: (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Final normalization and linear projection\n",
        "        normalized_outputs = self.norm_out(outputs_tensor)\n",
        "        logits = self.output_linear(normalized_outputs)\n",
        "\n",
        "        return logits\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "opmilAGU-qtV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXlOYhHy-qtW"
      },
      "source": [
        "## 3. Forward Pass Demonstration\n",
        "\n",
        "Let's create a dummy input sequence and pass it through our model to see the shapes of the tensors at each step. This helps verify that our implementation is working correctly.\n"
      ],
      "id": "kXlOYhHy-qtW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiY0bOmF-qtW",
        "outputId": "77461845-c392-4711-cc41-4e2c363821cb"
      },
      "source": [
        "# Model parameters\n",
        "VOCAB_SIZE = 20\n",
        "D_MODEL = 32\n",
        "SEQ_LEN = 5\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# Instantiate the model\n",
        "model = TRM(vocab_size=VOCAB_SIZE, d_model=D_MODEL)\n",
        "print(\"Model Architecture:\")\n",
        "print(model)\n",
        "\n",
        "# Create a dummy input sequence (batch_size, seq_len)\n",
        "# These are random token IDs from our vocabulary\n",
        "dummy_input = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LEN))\n",
        "print(f\"\\nDummy Input Shape: {dummy_input.shape}\")\n",
        "print(f\"Dummy Input Tensor:\\n{dummy_input}\")\n",
        "\n",
        "# Perform a forward pass\n",
        "output_logits = model(dummy_input)\n",
        "\n",
        "print(f\"\\nOutput Logits Shape: {output_logits.shape}\")\n",
        "print(\"This shape (batch_size, seq_len, vocab_size) is what we expect!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Architecture:\n",
            "TRM(\n",
            "  (embedding): Embedding(20, 32)\n",
            "  (recursive_block): RecursiveBlock(\n",
            "    (norm_state): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm_input): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (activation): GELU(approximate='none')\n",
            "    (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  )\n",
            "  (norm_out): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "  (output_linear): Linear(in_features=32, out_features=20, bias=True)\n",
            ")\n",
            "\n",
            "Dummy Input Shape: torch.Size([1, 5])\n",
            "Dummy Input Tensor:\n",
            "tensor([[10,  2,  4,  2,  8]])\n",
            "\n",
            "Output Logits Shape: torch.Size([1, 5, 20])\n",
            "This shape (batch_size, seq_len, vocab_size) is what we expect!\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "NiY0bOmF-qtW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0B8UsqA-qtX"
      },
      "source": [
        "## 4. A Simple Training Example\n",
        "\n",
        "To show that the model can learn, let's create a simple \"next token prediction\" task. Our goal is to train the model to predict the next number in a sequence.\n",
        "\n",
        "-   **Input:** `[1, 2, 3, 4]`\n",
        "-   **Target:** `[2, 3, 4, 5]`\n",
        "\n",
        "We'll use Cross-Entropy Loss and the Adam optimizer.\n"
      ],
      "id": "l0B8UsqA-qtX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxQ2ElMj-qtX",
        "outputId": "2816e139-6170-49f4-e090-a0a9a4a9e0a8"
      },
      "source": [
        "# --- Training Setup ---\n",
        "# Let's use a slightly bigger vocabulary for this task\n",
        "TRAINING_VOCAB_SIZE = 10\n",
        "TRAINING_D_MODEL = 16\n",
        "\n",
        "# Create a new model instance for training\n",
        "training_model = TRM(vocab_size=TRAINING_VOCAB_SIZE, d_model=TRAINING_D_MODEL)\n",
        "optimizer = optim.Adam(training_model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Data ---\n",
        "# Our simple sequence prediction task\n",
        "input_data = torch.tensor([[1, 2, 3, 4]])      # Shape: (1, 4)\n",
        "target_data = torch.tensor([[2, 3, 4, 5]])     # Shape: (1, 4)\n",
        "\n",
        "print(f\"Input:  {input_data[0].tolist()}\")\n",
        "print(f\"Target: {target_data[0].tolist()}\")\n",
        "\n",
        "# --- Training Loop ---\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    logits = training_model(input_data) # Shape: (batch, seq_len, vocab_size)\n",
        "\n",
        "    # Reshape for the loss function\n",
        "    # The loss function expects (N, C) where C is number of classes\n",
        "    # Logits: (1, 4, 10) -> (4, 10)\n",
        "    # Target: (1, 4) -> (4)\n",
        "    loss = criterion(logits.view(-1, TRAINING_VOCAB_SIZE), target_data.view(-1))\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# --- Inference after training ---\n",
        "print(\"\\n--- Testing after training ---\")\n",
        "with torch.no_grad():\n",
        "    test_input = torch.tensor([[1, 2, 3, 4]])\n",
        "    predictions = training_model(test_input)\n",
        "    # Get the predicted token ID by finding the max logit\n",
        "    predicted_ids = torch.argmax(predictions, dim=2)\n",
        "\n",
        "    print(f\"Input:           {test_input[0].tolist()}\")\n",
        "    print(f\"Predicted sequence: {predicted_ids[0].tolist()}\")\n",
        "    print(f\"Target sequence:    {target_data[0].tolist()}\")\n",
        "    print(\"\\nThe model has learned to predict the next number in the sequence!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  [1, 2, 3, 4]\n",
            "Target: [2, 3, 4, 5]\n",
            "Epoch [10/100], Loss: 0.3315\n",
            "Epoch [20/100], Loss: 0.0663\n",
            "Epoch [30/100], Loss: 0.0192\n",
            "Epoch [40/100], Loss: 0.0091\n",
            "Epoch [50/100], Loss: 0.0059\n",
            "Epoch [60/100], Loss: 0.0045\n",
            "Epoch [70/100], Loss: 0.0038\n",
            "Epoch [80/100], Loss: 0.0033\n",
            "Epoch [90/100], Loss: 0.0029\n",
            "Epoch [100/100], Loss: 0.0026\n",
            "\n",
            "--- Testing after training ---\n",
            "Input:           [1, 2, 3, 4]\n",
            "Predicted sequence: [2, 3, 4, 5]\n",
            "Target sequence:    [2, 3, 4, 5]\n",
            "\n",
            "The model has learned to predict the next number in the sequence!\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "vxQ2ElMj-qtX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X74k9M1-qtY"
      },
      "source": [
        "## 5. Advanced Example: Solving 20x20 Mazes\n",
        "\n",
        "Now for a more challenging task. Let's train the TRM to solve larger 20x20 mazes. This requires the model to process a much longer sequence (400 maze tokens + path tokens) and maintain its state to find the solution.\n",
        "\n",
        "**The Task:** The model will be given a sequence representing the maze structure, followed by the starting position of the path. Its goal is to predict the rest of the coordinate sequence that solves the maze.\n",
        "\n",
        "**Data Representation:**\n",
        "We will convert the maze and its solution path into a single sequence of integers (tokens).\n",
        "\n",
        "- **Vocabulary:**\n",
        "- `0`: Wall (`#`)\n",
        "- `1`: Path (`.`)\n",
        "- `2`: Start (`S`)\n",
        "- `3`: End (`E`)\n",
        "- `4`: Separator (`|`) - A special token to divide the maze layout from the path coordinates.\n",
        "- `5-404`: Path Coordinates - Each of the 400 cells `(row, col)` is mapped to a unique token `5 + row * 20 + col`.\n",
        "\n",
        "- **Sequence Format:**\n",
        "- The model is trained on a single continuous sequence. The input is the sequence up to step `n-1`, and the target is the sequence up to step `n` (shifted).\n",
        "- We only care about predicting the path. Therefore, we will use a **loss mask** to ignore the model's predictions for the maze layout part of the sequence during training.\n"
      ],
      "id": "8X74k9M1-qtY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtGKKtyj-qtY",
        "outputId": "7f1c351d-65c2-47c4-88ac-aca14a4e0ed8"
      },
      "source": [
        "# --- Maze Data Setup ---\n",
        "\n",
        "# Define a 20x20 maze and its solutions\n",
        "# 0: Wall, 1: Path, 2: Start, 3: End\n",
        "MAZE_DATASET = [\n",
        "    {\n",
        "        \"maze\": [\n",
        "            [2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "            [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "            [1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1],\n",
        "            [1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1],\n",
        "            [1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1],\n",
        "            [1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
        "            [1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
        "            [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1],\n",
        "            [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
        "            [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1],\n",
        "            [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1],\n",
        "            [1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1],\n",
        "            [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
        "            [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
        "            [1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
        "            [1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1],\n",
        "            [1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
        "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
        "            [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
        "            [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 3]\n",
        "        ],\n",
        "        \"path\": [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2), (3, 3), (4, 3), (4, 2), (5, 2), (5, 3), (5, 4), (5, 5), (6, 5), (7, 5), (7, 4), (7, 3), (7, 2), (7, 1), (8, 1), (9, 1), (9, 0), (10, 0), (11, 0), (11, 1), (11, 2), (11, 3), (12, 3), (13, 3), (13, 4), (13, 5), (14, 5), (15, 5), (15, 4), (15, 3), (16, 3), (17, 3), (17, 4), (17, 5), (17, 6), (17, 7), (17, 8), (17, 9), (18, 9), (19, 9), (19, 10), (19, 11), (19, 12), (19, 13), (18, 13), (17, 13), (16, 13), (15, 13), (14, 13), (13, 13), (12, 13), (11, 13), (10, 13), (9, 13), (8, 13), (7, 13), (6, 13), (5, 13), (4, 13), (3, 13), (2, 13), (1, 13), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (1, 19), (2, 19), (3, 19), (4, 19), (5, 19), (5, 18), (6, 18), (7, 18), (8, 18), (9, 18), (9, 17), (10, 17), (11, 17), (11, 18), (12, 18), (13, 18), (14, 18), (15, 18), (16, 18), (16, 19), (17, 19), (18, 19), (19, 19)]\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Vocabulary and Preprocessing ---\n",
        "WALL, PATH, START, END, SEP = 0, 1, 2, 3, 4\n",
        "PATH_TOKEN_OFFSET = 5\n",
        "MAZE_SIZE = 20 # <-- Updated size\n",
        "MAZE_TOKENS = MAZE_SIZE * MAZE_SIZE\n",
        "\n",
        "def preprocess_maze_data(dataset):\n",
        "    sequences = []\n",
        "    for item in dataset:\n",
        "        maze_flat = [token for row in item[\"maze\"] for token in row]\n",
        "        path_tokens = [PATH_TOKEN_OFFSET + r * MAZE_SIZE + c for r, c in item[\"path\"]]\n",
        "\n",
        "        full_sequence = maze_flat + [SEP] + path_tokens\n",
        "        sequences.append(torch.tensor(full_sequence))\n",
        "    return sequences\n",
        "\n",
        "training_sequences = preprocess_maze_data(MAZE_DATASET)\n",
        "\n",
        "# Let's inspect the first preprocessed sequence\n",
        "print(\"Original Maze (first example):\")\n",
        "for row in MAZE_DATASET[0]['maze']:\n",
        "    print(\"\".join([{0:'#', 1:'.', 2:'S', 3:'E'}[c] for c in row]))\n",
        "\n",
        "print(f\"\\nPreprocessed sequence length: {len(training_sequences[0])}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Maze (first example):\n",
            "S.#...#...#.........\n",
            "..#.#.#.#.#.#######.\n",
            ".##.#...#...#.....#.\n",
            "....#.#.#.#.#.###.#.\n",
            ".#.##.#...#...#...#.\n",
            ".#....###.#.#.#.#...\n",
            ".####.....#.#.#.#.#.\n",
            "......#.#...#...#.#.\n",
            "#.#.#.#.#.###.#.#.#.\n",
            "..#.#.#.#.....#.#...\n",
            ".##.#.#...#.#.#.###.\n",
            "....#...#.#.#.#.....\n",
            ".##.#.#.#.#.#.#.#.#.\n",
            "......#.#.....#.#.#.\n",
            ".####.#.#.#.#.#.#.#.\n",
            "....#.#...#.#...#.#.\n",
            ".##.#.###.#.#.#.#...\n",
            "..........#...#.#.#.\n",
            ".######.#.#.#.#.#.#.\n",
            "........#...#......E\n",
            "\n",
            "Preprocessed sequence length: 498\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "FtGKKtyj-qtY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUVi_zes-qtZ",
        "outputId": "b2592c65-6ebd-4add-8fae-76714df5571e"
      },
      "source": [
        "# --- Maze Model Training ---\n",
        "MAZE_VOCAB_SIZE = PATH_TOKEN_OFFSET + MAZE_TOKENS # 5 special tokens + 400 path tokens\n",
        "MAZE_D_MODEL = 64 # Increased model size for complexity\n",
        "lr = 0.001 # Use a smaller learning rate\n",
        "\n",
        "maze_model = TRM(vocab_size=MAZE_VOCAB_SIZE, d_model=MAZE_D_MODEL)\n",
        "maze_optimizer = optim.Adam(maze_model.parameters(), lr=lr)\n",
        "maze_criterion = nn.CrossEntropyLoss(reduction='none') # Use 'none' to apply mask later\n",
        "\n",
        "epochs = 2000 # Increased epochs for this harder task\n",
        "print(\"Starting maze training... (This will take a few minutes)\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for seq in training_sequences:\n",
        "        maze_optimizer.zero_grad()\n",
        "\n",
        "        # Prepare input and target (shifted input)\n",
        "        input_seq = seq[:-1].unsqueeze(0)\n",
        "        target_seq = seq[1:].unsqueeze(0)\n",
        "\n",
        "        # Define the loss mask: we only care about predicting the path tokens.\n",
        "        # The path starts after the maze layout (400 tokens) and the SEP token.\n",
        "        # The target for the SEP token is the first path token, so we start the mask there.\n",
        "        loss_mask = torch.zeros_like(target_seq, dtype=torch.float)\n",
        "        loss_mask[:, MAZE_TOKENS:] = 1.0\n",
        "\n",
        "        # Forward pass\n",
        "        logits = maze_model(input_seq)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = maze_criterion(logits.view(-1, MAZE_VOCAB_SIZE), target_seq.view(-1))\n",
        "        masked_loss = loss * loss_mask.view(-1)\n",
        "\n",
        "        # Average the loss over the number of path tokens only\n",
        "        final_loss = masked_loss.sum() / loss_mask.sum()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        final_loss.backward()\n",
        "        maze_optimizer.step()\n",
        "\n",
        "        total_loss += masked_loss.sum().item()\n",
        "        total_tokens += loss_mask.sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    if (epoch + 1) % 200 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Maze training finished.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting maze training... (This will take a few minutes)\n",
            "Epoch [200/2000], Average Loss: 0.0316\n",
            "Epoch [400/2000], Average Loss: 0.0098\n",
            "Epoch [600/2000], Average Loss: 0.0049\n",
            "Epoch [800/2000], Average Loss: 0.0030\n",
            "Epoch [1000/2000], Average Loss: 0.0020\n",
            "Epoch [1200/2000], Average Loss: 0.0014\n",
            "Epoch [1400/2000], Average Loss: 0.0011\n",
            "Epoch [1600/2000], Average Loss: 0.0008\n",
            "Epoch [1800/2000], Average Loss: 0.0007\n",
            "Epoch [2000/2000], Average Loss: 0.0005\n",
            "Maze training finished.\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "dUVi_zes-qtZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soj6H-GP-qtZ",
        "outputId": "edbd1d02-fe9d-487c-bc0d-c8cf2605468e"
      },
      "source": [
        "# --- Inference on a Maze ---\n",
        "def solve_maze(model, maze_grid, max_len=400): # Increased max_len\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    maze_flat = [token for row in maze_grid for token in row]\n",
        "\n",
        "    # Find start position to begin generation\n",
        "    start_pos_flat = maze_flat.index(START)\n",
        "    start_r, start_c = divmod(start_pos_flat, MAZE_SIZE)\n",
        "    start_path_token = PATH_TOKEN_OFFSET + start_r * MAZE_SIZE + start_c\n",
        "\n",
        "    # Initial input: maze layout + separator + start token\n",
        "    input_seq = torch.tensor(maze_flat + [SEP] + [start_path_token]).unsqueeze(0)\n",
        "\n",
        "    generated_path_tokens = [start_path_token]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - 1):\n",
        "            logits = model(input_seq)\n",
        "\n",
        "            # Get the prediction for the very last token in the sequence\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            predicted_token = torch.argmax(next_token_logits, dim=1).item()\n",
        "\n",
        "            # Append the prediction and update the input for the next step\n",
        "            generated_path_tokens.append(predicted_token)\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[predicted_token]])], dim=1)\n",
        "\n",
        "            # Stop if we predict the end token\n",
        "            end_pos_flat = maze_flat.index(END)\n",
        "            end_r, end_c = divmod(end_pos_flat, MAZE_SIZE)\n",
        "            end_path_token = PATH_TOKEN_OFFSET + end_r * MAZE_SIZE + end_c\n",
        "            if predicted_token == end_path_token:\n",
        "                break\n",
        "\n",
        "    # Convert token IDs back to (row, col) coordinates\n",
        "    path_coords = []\n",
        "    for token in generated_path_tokens:\n",
        "        flat_pos = token - PATH_TOKEN_OFFSET\n",
        "        r, c = divmod(flat_pos, MAZE_SIZE)\n",
        "        path_coords.append((r, c))\n",
        "\n",
        "    return path_coords\n",
        "\n",
        "# Test with the first maze from our dataset\n",
        "test_maze_grid = MAZE_DATASET[0][\"maze\"]\n",
        "predicted_path = solve_maze(maze_model, test_maze_grid)\n",
        "\n",
        "print(\"Maze to solve:\")\n",
        "for row in test_maze_grid:\n",
        "     print(\"\".join([{0:'#', 1:'.', 2:'S', 3:'E'}[c] for c in row]))\n",
        "\n",
        "print(f\"\\nPredicted Path has {len(predicted_path)} steps.\")\n",
        "print(f\"Correct Path has {len(MAZE_DATASET[0]['path'])} steps.\")\n",
        "\n",
        "# Visualize the path\n",
        "print(\"\\nVisualized solution (* = predicted path):\")\n",
        "solution_grid = [[' ' for _ in range(MAZE_SIZE)] for _ in range(MAZE_SIZE)]\n",
        "for r in range(MAZE_SIZE):\n",
        "    line = \"\"\n",
        "    for c in range(MAZE_SIZE):\n",
        "        if test_maze_grid[r][c] == WALL:\n",
        "            line += '#'\n",
        "        elif (r,c) in predicted_path:\n",
        "            line += '*'\n",
        "        else:\n",
        "            line += '.'\n",
        "    print(line)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maze to solve:\n",
            "S.#...#...#.........\n",
            "..#.#.#.#.#.#######.\n",
            ".##.#...#...#.....#.\n",
            "....#.#.#.#.#.###.#.\n",
            ".#.##.#...#...#...#.\n",
            ".#....###.#.#.#.#...\n",
            ".####.....#.#.#.#.#.\n",
            "......#.#...#...#.#.\n",
            "#.#.#.#.#.###.#.#.#.\n",
            "..#.#.#.#.....#.#...\n",
            ".##.#.#...#.#.#.###.\n",
            "....#...#.#.#.#.....\n",
            ".##.#.#.#.#.#.#.#.#.\n",
            "......#.#.....#.#.#.\n",
            ".####.#.#.#.#.#.#.#.\n",
            "....#.#...#.#...#.#.\n",
            ".##.#.###.#.#.#.#...\n",
            "..........#...#.#.#.\n",
            ".######.#.#.#.#.#.#.\n",
            "........#...#......E\n",
            "\n",
            "Predicted Path has 97 steps.\n",
            "Correct Path has 97 steps.\n",
            "\n",
            "Visualized solution (* = predicted path):\n",
            "*.#...#...#..*******\n",
            "*.#.#.#.#.#.#######*\n",
            "*##.#...#...#*....#*\n",
            "****#.#.#.#.#*###.#*\n",
            ".#*##.#...#..*#...#*\n",
            ".#****###.#.#*#.#.**\n",
            ".####*....#.#*#.#.#.\n",
            ".*****#.#...#*..#.#.\n",
            "#*#.#.#.#.###*#.#.#.\n",
            "**#.#.#.#....*#.#**.\n",
            "*##.#.#...#.#*#.###.\n",
            "****#...#.#.#*#..**.\n",
            ".##*#.#.#.#.#*#.#.#.\n",
            "...***#.#....*#.#.#.\n",
            ".####*#.#.#.#*#.#.#.\n",
            "...*#*#...#.#*..#.#.\n",
            ".##*#.###.#.#*#.#.**\n",
            "...*******#..*#.#.#*\n",
            ".######.#*#.#*#.#.#*\n",
            "........#***#*.....*\n"
          ]
        }
      ],
      "execution_count": null,
      "id": "soj6H-GP-qtZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxOxHXlV-qta"
      },
      "source": [
        "The model learns to generate the sequence of coordinates, effectively solving the maze. This demonstrates the TRM's ability to handle more structured and much longer sequence-to-sequence tasks that require understanding a context (the maze layout) to generate a relevant output (the path).\n"
      ],
      "id": "wxOxHXlV-qta"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lxwdLhO-qta"
      },
      "source": [
        "## 6. Key Differences from Transformers\n",
        "\n",
        "As summarized in the original repository, TRMs differ from Transformers in several key ways:\n",
        "\n",
        "-   **Computation:** TRMs are **recursive** and process sequences step-by-step, making their complexity linear, O(L). Transformers use **self-attention**, which is parallelizable but has quadratic complexity, O(LÂ²).\n",
        "-   **State Management:** TRMs explicitly manage a **hidden state** that evolves over time. Transformers are stateless and recompute context from scratch at each layer.\n",
        "-   **Positional Information:** TRMs inherently understand sequence order due to their sequential processing. Transformers require explicit **positional encodings** to be added to their inputs.\n"
      ],
      "id": "5lxwdLhO-qta"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TChc25Xj-qtb"
      },
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "This notebook provided a brief, practical introduction to Tiny Recursive Models. We implemented the core components in PyTorch, demonstrated that a simple TRM can learn a basic sequence prediction task, and showed its effectiveness on a more complex 20x20 maze-solving problem.\n",
        "\n",
        "TRMs offer an interesting alternative to Transformers, particularly for applications where computational efficiency and explicit state management are important.\n"
      ],
      "id": "TChc25Xj-qtb"
    }
  ]
}